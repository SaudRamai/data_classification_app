"""
AI Classification Pipeline Service

This service provides functionality for the Automatic AI Classification Pipeline sub-tab in the AI Assistant section.
It handles automated classification of data assets with semantic detection, CIA recommendations, and governance tagging.
"""

import logging
import math
import hashlib
import streamlit as st
from typing import List, Dict, Any, Optional
import pandas as pd
import numpy as np  # type: ignore
import re

try:
    from sentence_transformers import SentenceTransformer  # type: ignore
except Exception:
    SentenceTransformer = None  # type: ignore

from src.connectors.snowflake_connector import snowflake_connector
from src.services.ai_assistant_service import ai_assistant_service
from src.services.discovery_service import DiscoveryService
# AISensitiveDetectionService import moved to __init__ to avoid circular dependency
from src.services.governance_db_resolver import resolve_governance_db
from src.services.tagging_service import tagging_service
try:
    from src.config import settings
except Exception:
    settings = None  # type: ignore

logger = logging.getLogger(__name__)

class AIClassificationPipelineService:
    """Service for managing the automatic AI classification pipeline functionality."""

    def __init__(self):
        """Initialize the classification pipeline service."""
        self.ai_service = ai_assistant_service
        self.discovery = DiscoveryService()
        # Sensitive detection service is optional and initialized without AI linkage to avoid circular imports
        try:
            from src.services.ai_sensitive_detection_service import AISensitiveDetectionService
            self.sensitive_service = AISensitiveDetectionService(sample_size=200, min_confidence=0.3, use_ai=False)
        except Exception:
            self.sensitive_service = None
        # Disable governance glossary usage for semantic context (use only information_schema + config)
        try:
            setattr(self.ai_service, "use_governance_glossary", False)
        except Exception:
            pass
        # Local embedding backend (MiniLM) for governance-free detection
        self._embed_backend: str = 'none'
        self._embedder: Any = None
        self._category_centroids: Dict[str, Any] = {}
        self._category_tokens: Dict[str, List[str]] = {}
        # Tuning defaults
        self._w_sem: float = 0.85
        self._w_kw: float = 0.15
        self._ctx_max_cols: int = 12
        self._ctx_max_vals: int = 5
        self._col_sample_rows: int = 200
        self._conf_label_threshold: float = 0.30  # ENHANCED: Lowered from 0.45 to enable 80%+ confidence scores
        self._debug: bool = False
        self._cache: Dict[str, Any] = {}
        self._embed_cache: Dict[str, Any] = {}
        self._embed_ready: bool = False
        # Cached dynamic sensitivity configuration (patterns, keywords, categories, etc.)
        # This is populated externally (e.g. from 3_Classification or seed_governance_service)
        self._sensitivity_config: Dict[str, Any] = {}

    def render_classification_pipeline(self) -> None:
        """Render the Automatic AI Classification Pipeline sub-tab."""
        st.markdown("### Automatic AI Classification Pipeline")
        st.caption("Automatically discovers existing data assets, derives business context, performs semantic category detection, recommends CIA levels per Avendra's Data Classification Policy, and applies governance tags with full validation, review routing, and audit logging.")

        show_steps = False

        # Get active database
        db = self._get_active_database()
        if not db:
            st.info("Use Global Filters to select a database to run the pipeline.")
            return

        # Additional validation
        if db.upper() in ('NONE', '(NONE)', 'NULL', 'UNKNOWN', ''):
            st.error("Invalid database selected. Please choose a valid database.")
            return

        # Get governance database
        gov_db = self._get_governance_database(db)

        # Show status and set Snowflake context
        try:
            schema_filter = self._get_schema_filter()
            table_filter = self._get_table_filter()
        except Exception:
            schema_filter = None; table_filter = None
        st.caption(
            f"Using Database: {db}"
            + (f" | Schema: {schema_filter}" if schema_filter else "")
            + (f" | Table filter: {table_filter}" if table_filter else "")
        )
        try:
            snowflake_connector.execute_non_query(f"USE DATABASE {db}")
        except Exception:
            pass
        try:
            self._auto_tune_parameters()
        except Exception:
            pass

        # Removed automatic background pipeline as it's unused and causes errors

        if show_steps:
            with st.expander("Step 2: Collect Metadata", expanded=False):
                colm1, colm2 = st.columns([1, 1])
                with colm1:
                    if st.button("Collect table & column metadata", key="btn_collect_meta"):
                        try:
                            tables_df, columns_df, glossary_df = self._collect_metadata(db, schema_filter, table_filter, gov_db)
                            st.session_state["pipe_meta_tables"] = tables_df
                            st.session_state["pipe_meta_columns"] = columns_df
                            st.session_state["pipe_meta_glossary"] = glossary_df
                        except Exception as me:
                            st.error(f"Metadata collection failed: {me}")
                with colm2:
                    st.caption("Collects information_schema tables/columns, plus governance glossary references.")

            try:
                tables_df = st.session_state.get("pipe_meta_tables") if hasattr(st, 'session_state') else None
                columns_df = st.session_state.get("pipe_meta_columns") if hasattr(st, 'session_state') else None
                glossary_df = st.session_state.get("pipe_meta_glossary") if hasattr(st, 'session_state') else None
            except Exception:
                tables_df = columns_df = glossary_df = None

            if isinstance(tables_df, pd.DataFrame) and not tables_df.empty:
                st.markdown("**Tables (preview)**")
                st.dataframe(tables_df.head(100), width='stretch', hide_index=True)

            if isinstance(columns_df, pd.DataFrame) and not columns_df.empty:
                st.markdown("**Columns (preview)**")
                st.dataframe(columns_df.head(200), width='stretch', hide_index=True)

                # Sampling UI
                try:
                    sample_cols = columns_df[["TABLE_SCHEMA", "TABLE_NAME", "COLUMN_NAME"]].drop_duplicates()
                    sample_cols["FQN"] = sample_cols.apply(lambda r: f"{db}.{r['TABLE_SCHEMA']}.{r['TABLE_NAME']} :: {r['COLUMN_NAME']}", axis=1)
                    sel = st.selectbox("Sample column values", options=[""] + list(sample_cols["FQN"]), key="pipe_sample_pick")
                    if sel:
                        try:
                            parts = sel.split(" :: ")
                            tblfqn = parts[0]
                            colname = parts[1]
                            _, sch, tbl = tblfqn.split('.')
                            values = self._sample_column_values(db=db, schema=sch, table=tbl, column=colname, sample_rows=100)
                            if values:
                                st.markdown("**Sample Values**")
                                st.write(values[:50])
                            else:
                                st.caption("No non-null values sampled.")
                        except Exception as se:
                            st.warning(f"Sampling failed: {se}")
                except Exception:
                    pass

            if isinstance(glossary_df, pd.DataFrame) and not glossary_df.empty:
                st.markdown("**Glossary / Compliance Mapping**")
                st.dataframe(glossary_df.head(200), width='stretch', hide_index=True)

        if show_steps:
            with st.expander("Step 3: Semantic Embedding Generation", expanded=False):
                c1, c2 = st.columns([1, 1])
                with c1:
                    if st.button("Initialize embedding backend", key="btn_init_embed"):
                        try:
                            emb_info = self.ai_service.initialize_sensitive_detection()
                            if hasattr(st, 'session_state'):
                                st.session_state["pipe_embed_info"] = emb_info
                            st.success(f"Backend: {emb_info.get('backend')} | Categories: {', '.join(emb_info.get('categories', []))}")
                        except Exception as ee:
                            st.error(f"Embedding init failed: {ee}")
                with c2:
                    try:
                        emb_info = st.session_state.get("pipe_embed_info") if hasattr(st, 'session_state') else None
                        if emb_info:
                            st.caption(f"Backend: {emb_info.get('backend')} | Categories loaded: {len(emb_info.get('categories', []))}")
                    except Exception:
                        pass

                try:
                    cols_df = st.session_state.get("pipe_meta_columns") if hasattr(st, 'session_state') else None
                except Exception:
                    cols_df = None

                cc1, cc2 = st.columns([1, 1])
                with cc1:
                    if st.button("Embed column contexts (sample)", key="btn_embed_cols"):
                        try:
                            out_rows = []
                            if isinstance(cols_df, pd.DataFrame) and not cols_df.empty:
                                sample = cols_df.head(100).copy()
                                for _, r in sample.iterrows():
                                    ctx = f"{db}.{r.get('TABLE_SCHEMA')}.{r.get('TABLE_NAME')}.{r.get('COLUMN_NAME')}"
                                    com = r.get('COLUMN_COMMENT') or ''
                                    if com:
                                        ctx = f"{ctx} | {com}"
                                    vec = None
                                    try:
                                        if hasattr(self.ai_service, "_get_embedding"):
                                            vec = self.ai_service._get_embedding(ctx)
                                    except Exception:
                                        vec = None
                                    dim = int(getattr(vec, "shape", [0])[-1]) if vec is not None and hasattr(vec, 'shape') else (len(vec) if isinstance(vec, (list, tuple)) else 0)
                                    out_rows.append({
                                        "context": ctx,
                                        "dim": dim,
                                    })
                            dfv = pd.DataFrame(out_rows)
                            if hasattr(st, 'session_state'):
                                st.session_state["pipe_embed_columns"] = dfv
                        except Exception as ce:
                            st.error(f"Column embedding failed: {ce}")
                with cc2:
                    if st.button("Build category reference embeddings", key="btn_embed_cats"):
                        try:
                            info = self.ai_service.initialize_sensitive_detection()
                            cents = getattr(self.ai_service, "_category_centroids", {}) or {}
                            rows = []
                            for k, v in cents.items():
                                dim = int(getattr(v, "shape", [0])[-1]) if v is not None and hasattr(v, 'shape') else (len(v) if isinstance(v, (list, tuple)) else 0)
                                rows.append({"category": k, "dim": dim, "has_centroid": v is not None})
                            dfr = pd.DataFrame(rows)
                            if hasattr(st, 'session_state'):
                                st.session_state["pipe_embed_categories"] = dfr
                        except Exception as ce2:
                            st.error(f"Category embedding failed: {ce2}")

                try:
                    dfv = st.session_state.get("pipe_embed_columns") if hasattr(st, 'session_state') else None
                    if isinstance(dfv, pd.DataFrame) and not dfv.empty:
                        st.markdown("**Embedded Column Contexts (sample)**")
                        st.dataframe(dfv.head(50), width='stretch', hide_index=True)
                except Exception:
                    pass

                try:
                    dfr = st.session_state.get("pipe_embed_categories") if hasattr(st, 'session_state') else None
                    if isinstance(dfr, pd.DataFrame) and not dfr.empty:
                        st.markdown("**Category Reference Embeddings**")
                        st.dataframe(dfr, width='stretch', hide_index=True)
                except Exception:
                    pass

        if show_steps:
            with st.expander("Step 4: Similarity Matching", expanded=False):
                thr = st.slider("Confidence threshold (cosine)", min_value=0.3, max_value=0.95, value=0.45, step=0.05, key="pipe_sim_thr")
                run_btn = st.button("Compute best category per column (sample)", key="btn_compute_similarity")
                cols_df = None
                try:
                    cols_df = st.session_state.get("pipe_meta_columns") if hasattr(st, 'session_state') else None
                except Exception:
                    cols_df = None
                if run_btn:
                    try:
                        cents = getattr(self.ai_service, "_category_centroids", {}) or {}
                        if not cents:
                            _ = self.ai_service.initialize_sensitive_detection()
                            cents = getattr(self.ai_service, "_category_centroids", {}) or {}
                        out_rows: List[Dict[str, Any]] = []
                        if isinstance(cols_df, pd.DataFrame) and not cols_df.empty and cents:
                            sample = cols_df.head(100).copy()
                            for _, r in sample.iterrows():
                                ctx = f"{db}.{r.get('TABLE_SCHEMA')}.{r.get('TABLE_NAME')}.{r.get('COLUMN_NAME')}"
                                com = r.get('COLUMN_COMMENT') or ''
                                if com:
                                    ctx = f"{ctx} | {com}"
                                v = None
                                try:
                                    if hasattr(self.ai_service, "_get_embedding"):
                                        v = self.ai_service._get_embedding(ctx)
                                except Exception:
                                    v = None
                                best_cat = None
                                best_sim = 0.0
                                if v is not None and np is not None:
                                    try:
                                        vv = np.asarray(v, dtype=float)
                                        nv = float(np.linalg.norm(vv) or 0.0)
                                        if nv > 0:
                                            vv = vv / nv
                                        for cat, c in cents.items():
                                            if c is None:
                                                continue
                                            cc = np.asarray(c, dtype=float)
                                            nc = float(np.linalg.norm(cc) or 0.0)
                                            if nc > 0:
                                                cc = cc / nc
                                            sim = float(np.dot(vv, cc))
                                            if sim > best_sim:
                                                best_sim = sim
                                                best_cat = cat
                                    except Exception:
                                        pass
                                assigned = best_cat if (best_cat and best_sim >= thr) else None
                                out_rows.append({
                                    "Schema": r.get('TABLE_SCHEMA'),
                                    "Table": r.get('TABLE_NAME'),
                                    "Column": r.get('COLUMN_NAME'),
                                    "BestCategory": best_cat or "",
                                    "Similarity": round(best_sim, 4),
                                    "Assigned": assigned or "",
                                })
                        df_sim = pd.DataFrame(out_rows)
                        if hasattr(st, 'session_state'):
                            st.session_state["pipe_similarity_results"] = df_sim
                    except Exception as se:
                        st.error(f"Similarity matching failed: {se}")

                try:
                    df_sim = st.session_state.get("pipe_similarity_results") if hasattr(st, 'session_state') else None
                    if isinstance(df_sim, pd.DataFrame) and not df_sim.empty:
                        st.markdown("**Similarity Results (sample)**")
                        st.dataframe(df_sim, width='stretch', hide_index=True)
                except Exception:
                    pass

            try:
                df_sim = st.session_state.get("pipe_similarity_results") if hasattr(st, 'session_state') else None
                if isinstance(df_sim, pd.DataFrame) and not df_sim.empty:
                    st.markdown("**Similarity Results (sample)**")
                    st.dataframe(df_sim, width='stretch', hide_index=True)
            except Exception:
                pass

        if show_steps:
            with st.expander("Debug: Discovery preview", expanded=False):
                st.write({
                    "database": db,
                    "schema_filter": schema_filter,
                    "table_filter": table_filter,
                })
                try:
                    sample_assets = self._discover_assets(db)[:10]
                    if sample_assets:
                        st.dataframe(pd.DataFrame(sample_assets), width='stretch', hide_index=True)
                    else:
                        st.caption("No assets discovered with current filters.")
                except Exception as _de:
                    st.caption(f"Discovery preview error: {_de}")

        # Run pipeline button
        if st.button("Run AI Classification Pipeline", type="primary", key="run_ai_pipeline"):
            self._run_classification_pipeline(db, gov_db)

    def _get_active_database(self) -> Optional[str]:
        """Get the active database from global filters."""
        try:
            from src.pages.page_helpers import _active_db_from_filter
            db = _active_db_from_filter()
        except Exception:
            db = None

        # Fallbacks from session state
        if not db:
            try:
                if hasattr(st, "session_state"):
                    db = (
                        st.session_state.get("sf_database")
                        or (st.session_state.get("global_filters", {}) or {}).get("database")
                        or st.session_state.get("rt_db")
                        or st.session_state.get("selected_database")
                    )
            except Exception:
                db = None

        # Settings fallback
        if not db and settings is not None:
            try:
                db = getattr(settings, "SNOWFLAKE_DATABASE", None)
            except Exception:
                db = None

        # Probe Snowflake context last
        if not db:
            try:
                rows = snowflake_connector.execute_query("select current_database() as DB") or []
                if rows and rows[0].get("DB"):
                    db = rows[0].get("DB")
            except Exception:
                db = None

        try:
            dbu = str(db or "").strip().upper()
            if dbu in {"", "NONE", "(NONE)", "NULL", "UNKNOWN"}:
                return None
        except Exception:
            pass

        return db

    def _is_valid_database(self, db: str) -> bool:
        """Check if database name is valid (not placeholder/invalid)."""
        try:
            db_upper = str(db or '').upper()
            invalid_names = {'', 'NONE', '(NONE)', 'NULL', 'UNKNOWN'}
            return db_upper not in invalid_names
        except Exception:
            return False

    def _get_governance_database(self, db: str) -> str:
        """Resolve governance database for CTE joins."""
        try:
            gov_db = resolve_governance_db() or db
            return gov_db
        except Exception:
            return db

    def _run_classification_pipeline(self, db: str, gov_db: str) -> None:
        """Execute the full AI classification pipeline."""
        # Validate database
        if not db or db.upper() in ('NONE', '(NONE)', 'NULL', 'UNKNOWN', ''):
            st.error("Invalid database selected. Please choose a valid database from Global Filters.")
            return

        # Validate governance database
        if gov_db and gov_db.upper() in ('NONE', '(NONE)', 'NULL', 'UNKNOWN'):
            gov_db = db  # fallback to main db

        with st.spinner("Running AI Classification Pipeline... This may take several minutes."):
            try:
                # Prefer live metadata for discovery, but classification will be governance-free
                try:
                    self.ai_service.use_snowflake = True
                except Exception:
                    pass

                # Initialize local MiniLM (or fallback) embeddings and category centroids
                self._init_local_embeddings()

                try:
                    self._auto_tune_parameters()
                except Exception:
                    pass

                # Step 1b: Local discovery list for this run (preview)
                assets = self._discover_assets(db)
                if not assets:
                    st.warning("No tables found in the selected database.")
                    return

                st.info(f"Discovered {len(assets)} tables for classification.")

                # Show backend status
                st.caption(f"Embedding backend: {self._embed_backend}")
                st.caption(
                    "Detection mode: "
                    + ("MiniLM embeddings + keyword fallback" if self._embed_backend == 'sentence-transformers' else "keyword/regex fallback only")
                )
                try:
                    cat_cnt = len(getattr(self, "_category_centroids", {}) or {})
                    tok_cnt = sum(len(v) for v in (getattr(self, "_category_tokens", {}) or {}).values())
                    st.caption(f"Categories loaded: {cat_cnt} | Tokens configured: {tok_cnt}")
                    if self._embed_backend == 'sentence-transformers' and cat_cnt == 0:
                        st.warning("MiniLM is active but no category centroids are available. Falling back to keywords only. Verify governance config and embeddings initialization.")
                except Exception:
                    pass

                # Step 2-8: Run local classification pipeline (no governance tables)
                results = self._classify_assets_local(db=db, assets=assets[:50])

                try:
                    conf_list = [float(r.get('confidence', 0.0) or 0.0) for r in results if isinstance(r, dict) and 'error' not in r]
                    adaptive = None
                    if conf_list:
                        if np is not None:
                            try:
                                p = float(np.percentile(conf_list, 60))
                            except Exception:
                                p = float(sum(conf_list)/max(1,len(conf_list)))
                        else:
                            try:
                                sl = sorted(conf_list)
                                idx = int(round(0.6 * (len(sl)-1)))
                                p = float(sl[idx])
                            except Exception:
                                p = float(sum(conf_list)/max(1,len(conf_list)))
                        # Do not lower below configured threshold; avoid circular lowering
                        base_thr = float(getattr(self, "_conf_label_threshold", 0.5) or 0.5)
                        adaptive = max(base_thr, min(0.75, p))
                    if adaptive is not None:
                        lbl_map = {
                            'Confidential': 'ðŸŸ¥ Confidential',
                            'Restricted': 'ðŸŸ§ Restricted',
                            'Internal': 'ðŸŸ¨ Internal',
                            'Public': 'ðŸŸ© Public',
                            'Uncertain â€” review': 'â¬œ Uncertain â€” review',
                        }
                        col_map = {
                            'Confidential': 'Red',
                            'Restricted': 'Orange',
                            'Internal': 'Yellow',
                            'Public': 'Green',
                            'Uncertain â€” review': 'Gray',
                        }
                        for r in results:
                            try:
                                if 'error' in r:
                                    continue
                                confv = float(r.get('confidence', 0.0) or 0.0)
                                base_label = str(r.get('label') or '')
                                if confv < adaptive:
                                    r['label'] = 'Uncertain â€” review'
                                    r['label_emoji'] = lbl_map['Uncertain â€” review']
                                    r['color'] = col_map['Uncertain â€” review']
                                else:
                                    # ensure emoji/color match base_label
                                    r['label_emoji'] = lbl_map.get(base_label, base_label)
                                    r['color'] = col_map.get(base_label, r.get('color', ''))
                            except Exception:
                                continue
                except Exception:
                    pass

                if not results:
                    st.warning("No assets were successfully classified.")
                    return

                # Display results (table-level only)
                self._display_classification_results(results)

                # Summary
                successful = len([r for r in results if 'error' not in r])
                failed = len([r for r in results if 'error' in r])

                st.success(f"Pipeline completed! Successfully classified {successful} assets. Failed: {failed}")

                if failed > 0:
                    with st.expander("View Errors", expanded=False):
                        for result in results:
                            if 'error' in result:
                                st.error(f"{result['asset']['full_name']}: {result['error']}")

            except Exception as e:
                logger.error(f"Pipeline execution failed: {e}")
                st.error(f"Pipeline failed: {e}")

    def _init_local_embeddings(self) -> None:
        try:
            if not hasattr(self, "_embed_cache") or not isinstance(self._embed_cache, dict):
                self._embed_cache = {}
            self._embed_ready = False
            if SentenceTransformer is not None:
                try:
                    logger.info("Initializing SentenceTransformer embeddings...")
                    self._embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
                    tv = self._embedder.encode(["ok"], normalize_embeddings=True)
                    v0 = tv[0] if isinstance(tv, (list, tuple)) else tv
                    dim = int(getattr(v0, "shape", [0])[-1]) if hasattr(v0, "shape") else (len(v0) if isinstance(v0, (list, tuple)) else 0)
                    if dim and dim > 0:
                        self._embed_backend = 'sentence-transformers'
                        self._embed_ready = True
                        logger.info(f"âœ“ Embeddings initialized successfully. Backend: {self._embed_backend}, Dimension: {dim}")
                    else:
                        self._embedder = None
                        self._embed_backend = 'none'
                        logger.warning(f"âœ— Embedding dimension validation failed: {dim}")
                except Exception as _e:
                    try:
                        logger.warning(f"âœ— Local embedding initialization failed: {_e}")
                    except Exception:
                        pass
                    self._embedder = None
                    self._embed_backend = 'none'
            else:
                self._embedder = None
                self._embed_backend = 'none'
                logger.warning("âœ— SentenceTransformer not available")
        except Exception as _e2:
            try:
                logger.warning(f"âœ— Embedding setup error: {_e2}")
            except Exception:
                pass
            self._embedder = None
            self._embed_backend = 'none'
            try:
                self._embed_ready = False
            except Exception:
                pass

        self._category_centroids = {}
        self._category_tokens = {}
        self._category_patterns: Dict[str, List[str]] = {}
        schema_fqn = "DATA_CLASSIFICATION_GOVERNANCE"
        gov_db = None
        try:
            gov_db = resolve_governance_db()
        except Exception:
            gov_db = None
        if gov_db:
            try:
                snowflake_connector.execute_non_query(f"USE DATABASE {gov_db}")
            except Exception:
                pass
            schema_fqn = f"{gov_db}.DATA_CLASSIFICATION_GOVERNANCE"
        rows = []
        try:
            rows = snowflake_connector.execute_query(
                f"""
                SELECT 
                    COALESCE(category_name, category, name) AS CATEGORY_NAME,
                    COALESCE(description, desc, details) AS DESCRIPTION,
                    COALESCE(is_active, true) AS IS_ACTIVE
                FROM {schema_fqn}.SENSITIVITY_CATEGORIES
                WHERE COALESCE(is_active, true)
                """
            ) or []
        except Exception:
            try:
                rows = snowflake_connector.execute_query(
                    f"""
                    SELECT 
                        COALESCE(category_name, category, name) AS CATEGORY_NAME,
                        COALESCE(description, desc, details) AS DESCRIPTION,
                        COALESCE(is_active, true) AS IS_ACTIVE
                    FROM {schema_fqn}.SENSITIVITY_CATEGORIES
                    WHERE COALESCE(is_active, true)
                    """
                ) or []
            except Exception:
                rows = []
        cats = []
        for r in rows:
            try:
                n = str(r.get("CATEGORY_NAME") or "").strip()
                d = str(r.get("DESCRIPTION") or "").strip()
                if n:
                    cats.append((n, d))
            except Exception:
                continue
        if not cats and self._embed_backend == 'sentence-transformers':
            self._create_fallback_categories()
            return
        if self._embed_backend != 'sentence-transformers' or self._embedder is None or np is None:
            self._create_fallback_categories()
            return
        centroids: Dict[str, Any] = {}
        tokens_out: Dict[str, List[str]] = {}
        for name, desc in cats:
            try:
                ex = self._generate_category_examples(name, desc)
            except Exception:
                ex = [name]
            try:
                kw_for_ex = self._load_additional_tokens_from_keywords(schema_fqn, name)
            except Exception:
                kw_for_ex = []
            if kw_for_ex:
                try:
                    add = []
                    for k in kw_for_ex[:8]:
                        ks = str(k).strip()
                        if ks:
                            add.append(ks)
                            add.append(f"contains {ks}")
                    if add:
                        base_seen = set([s.lower() for s in ex])
                        for a in add:
                            al = a.lower()
                            if al not in base_seen:
                                ex.append(a)
                                base_seen.add(al)
                except Exception:
                    pass
            try:
                # preprocess examples to reduce noise and expand abbreviations
                ex2 = [self._preprocess_text_local(s, remove_stopwords=True) for s in ex]
                vecs = self._embedder.encode(ex2, normalize_embeddings=True)
                mat = np.stack(vecs, axis=0)
                c = np.mean(mat, axis=0)
                n = float(np.linalg.norm(c) or 0.0)
                if n > 0:
                    c = c / n
                centroids[name] = c
            except Exception:
                centroids[name] = None
            try:
                toks = self._generate_category_tokens(name, desc)
            except Exception:
                toks = []
            try:
                extra = self._load_additional_tokens_from_keywords(schema_fqn, name)
                if extra:
                    toks.extend(extra)
            except Exception:
                pass
            if toks:
                seen = set(); out = []
                for t in toks:
                    tt = re.sub(r"[_\-]+", " ", str(t)).strip()
                    tl = tt.lower()
                    if tt and tl not in seen:
                        out.append(tt)
                        seen.add(tl)
                tokens_out[name] = out
            # load patterns from governance for this category
            try:
                pats = self._load_patterns_from_governance(schema_fqn, name)
            except Exception:
                pats = []
            if pats:
                self._category_patterns[name] = pats
        self._category_centroids = centroids
        self._category_tokens = tokens_out
        
        # Diagnostic logging for centroid generation
        valid_centroids = len({k: v for k, v in centroids.items() if v is not None})
        total_tokens = sum(len(v) for v in tokens_out.values())
        logger.info(f"Centroid generation complete: {valid_centroids} valid centroids, {total_tokens} total tokens")
        logger.info(f"  Backend: {self._embed_backend}, Ready: {self._embed_ready}")
        logger.info(f"  Categories with centroids: {[k for k, v in centroids.items() if v is not None]}")
        logger.info(f"  Categories with tokens: {list(tokens_out.keys())}")
        
        try:
            st.session_state["_pipe_cat_count"] = valid_centroids
            st.session_state["_pipe_tok_count"] = total_tokens
        except Exception:
            pass

    def _generate_category_examples(self, name: str, desc: str) -> List[str]:
        n = (name or "").strip()
        d = (desc or "").strip()
        out: List[str] = []
        
        # Base examples: full name and description
        if n:
            out.append(n)
        if d:
            out.append(d)

        # Extract meaningful tokens from name/description
        raw_tokens: List[str] = []
        for s in [n, d]:
            for w in re.split(r"[^a-zA-Z0-9]+", s):
                w2 = w.strip()
                if not w2:
                    continue
                if len(w2) < 3:
                    continue
                raw_tokens.append(w2)

        # Conservative stopword filter (preserve domain-critical terms)
        # Removed: "data", "info", "information" (domain-critical)
        stops = {"the","a","an","and","or","of","to","in","for","on","at","by","with","from","as","is","are","was","were"}
        toks: List[str] = []
        seen: set = set()
        for t in raw_tokens:
            tl = t.lower()
            if tl in stops:
                continue
            if tl not in seen:
                toks.append(t)
                seen.add(tl)

        # Build enriched examples with multiple phrase patterns
        phrases: List[str] = []
        for t in toks[:12]:  # Increased from 10
            phrases.append(t)
            phrases.append(f"contains {t}")
            phrases.append(f"{t} field")
            phrases.append(f"{t} column")
        
        # Add domain-specific terminology patterns
        domain_patterns: List[str] = []
        for t in toks[:8]:
            domain_patterns.append(f"{t} record")
            domain_patterns.append(f"{t} value")
            domain_patterns.append(f"{t} attribute")
        
        # Combine all examples
        ex = out + phrases + domain_patterns
        
        # Dedupe while preserving order
        seen2 = set()
        dedup: List[str] = []
        for s in ex:
            sl = s.lower().strip()
            if sl and sl not in seen2:
                dedup.append(s.strip())
                seen2.add(sl)
        
        return dedup[:64]

    def _generate_category_tokens(self, name: str, desc: str) -> List[str]:
        n = (name or "").strip()
        d = (desc or "").strip()
        raw: List[str] = []
        for s in [n, d]:
            for w in re.split(r"[^a-zA-Z0-9]+", s):
                w2 = w.strip()
                if not w2:
                    continue
                if len(w2) < 3:
                    continue
                raw.append(w2)
        # simple stopword removal and dedupe
        stops = {"the","a","an","and","or","of","to","in","for","on","at","by","with","from","as","is","are","was","were","data","info","information"}
        out: List[str] = []
        seen = set()
        for t in raw:
            tt = re.sub(r"[_\-]+", " ", str(t)).strip()
            tl = tt.lower()
            if not tt or tl in stops:
                continue
            if tl not in seen:
                out.append(tt)
                seen.add(tl)
        return out[:64]

    def _load_additional_tokens_from_keywords(self, schema_fqn: str, category_name: str) -> List[str]:
        rows = []
        try:
            rows = snowflake_connector.execute_query(
                f"""
                SELECT COALESCE(keyword, KEYWORD, KEYWORD_STRING) AS KW
                FROM {schema_fqn}.SENSITIVE_KEYWORDS
                WHERE COALESCE(is_active, true) AND (
                    LOWER(category) = LOWER(%(n)s) OR LOWER(category_name) = LOWER(%(n)s)
                )
                """,
                {"n": category_name},
            ) or []
        except Exception:
            try:
                # CATEGORY_ID-based lookup using SENSITIVITY_CATEGORIES
                rows = snowflake_connector.execute_query(
                    f"""
                    SELECT COALESCE(keyword, KEYWORD, KEYWORD_STRING) AS KW
                    FROM {schema_fqn}.SENSITIVE_KEYWORDS
                    WHERE COALESCE(is_active, true) AND category_id IN (
                        SELECT CATEGORY_ID FROM {schema_fqn}.SENSITIVITY_CATEGORIES WHERE LOWER(CATEGORY_NAME) = LOWER(%(n)s)
                    )
                    """,
                    {"n": category_name},
                ) or []
            except Exception:
                try:
                    rows = snowflake_connector.execute_query(
                        f"""
                        SELECT COALESCE(keyword, KEYWORD, KEYWORD_STRING) AS KW
                        FROM {schema_fqn}.SENSITIVITY_KEYWORDS
                        WHERE COALESCE(is_active, true) AND category_id IN (
                            SELECT CATEGORY_ID FROM {schema_fqn}.SENSITIVITY_CATEGORIES WHERE LOWER(CATEGORY_NAME) = LOWER(%(n)s)
                        )
                        """,
                        {"n": category_name},
                    ) or []
                except Exception:
                    rows = []
        out: List[str] = []
        for r in rows:
            try:
                kw = str(r.get("KW") or "").strip()
                if kw:
                    out.append(kw)
            except Exception:
                continue
        return out

    def _create_fallback_categories(self) -> None:
        cats = [
            ("PERSONAL_DATA", "personal data pii name email phone address ssn passport dob identifier customer individual"),
            ("FINANCIAL_DATA", "financial salary payroll account bank credit debit transaction ledger revenue expense invoice payment"),
            ("REGULATORY_DATA", "regulatory compliance gdpr ccpa hipaa pci law regulation consent data subject rights"),
            ("PROPRIETARY_DATA", "proprietary trade secret intellectual property confidential design source code roadmap pricing"),
            ("INTERNAL_DATA", "internal business operational non sensitive standard general reference lookup config"),
            ("PUBLIC_DATA", "public open shared non confidential non sensitive"),
        ]
        centroids: Dict[str, Any] = {}
        tokens_out: Dict[str, List[str]] = {}
        for name, desc in cats:
            try:
                toks = self._generate_category_tokens(name, desc)
            except Exception:
                toks = []
            if toks:
                tokens_out[name] = toks
            try:
                if self._embedder is not None and np is not None and self._embed_backend == 'sentence-transformers':
                    ex = self._generate_category_examples(name, desc)
                    ex2 = [self._preprocess_text_local(s, remove_stopwords=True) for s in ex]
                    vecs = self._embedder.encode(ex2, normalize_embeddings=True)
                    mat = np.stack(vecs, axis=0)
                    c = np.mean(mat, axis=0)
                    n = float(np.linalg.norm(c) or 0.0)
                    if n > 0:
                        c = c / n
                    centroids[name] = c
                else:
                    centroids[name] = None
            except Exception:
                centroids[name] = None
        self._category_centroids = centroids
        self._category_tokens = tokens_out
        try:
            st.session_state["_pipe_cat_count"] = len({k: v for k, v in centroids.items() if v is not None})
            st.session_state["_pipe_tok_count"] = sum(len(v) for v in tokens_out.values())
        except Exception:
            pass

    def _semantic_scores(self, text: str) -> Dict[str, float]:
        """Compute semantic similarity scores per category in [0,1] using local centroids."""
        scores: Dict[str, float] = {}
        if not text:
            return scores
        if self._embedder is None or not self._category_centroids:
            return scores
        try:
            t = str(text or "")
            key = f"emb::pp1::{t}"
            v = self._embed_cache.get(key) if hasattr(self, "_embed_cache") else None
            if v is None:
                v_raw = self._embedder.encode([t], normalize_embeddings=True)
                v = np.asarray(v_raw[0], dtype=float)
                try:
                    if hasattr(self, "_embed_cache"):
                        self._embed_cache[key] = v
                except Exception:
                    pass
            n = float(np.linalg.norm(v) or 0.0)
            if n > 0:
                v = v / n

            raw: Dict[str, float] = {}
            for cat, c in self._category_centroids.items():
                try:
                    if c is None:
                        continue
                    sim = float(np.dot(v, c))
                    conf = max(0.0, min(1.0, (sim + 1.0) / 2.0))
                    raw[cat] = conf
                except Exception:
                    continue

            if not raw:
                return {}
            try:
                vals = list(raw.values())
                mn = min(vals)
                mx = max(vals)
                if mx > mn:
                    for k, v0 in raw.items():
                        x = (float(v0) - float(mn)) / (float(mx) - float(mn))
                        # ENHANCED: Boost high-confidence scores to reach 80%+ confidence
                        # Remove artificial cap at 0.7; allow scores to reach 0.95+
                        if x >= 0.7:
                            # Very aggressive boost for very strong signals: x^0.3
                            x = pow(x, 0.3)
                        elif x >= 0.6:
                            # Aggressive boost for strong signals: x^0.4 amplifies separation
                            x = pow(x, 0.4)
                        elif x >= 0.4:
                            # Moderate boost for medium signals
                            x = pow(x, 0.6)
                        elif x >= 0.2:
                            # Gentle boost for weak signals
                            x = pow(x, 0.8)
                        scores[k] = max(0.0, min(1.0, x))
                else:
                    scores = dict(raw)
            except Exception:
                scores = dict(raw)
        except Exception:
            return {}
        return scores

    def _get_semantic_matches_gov(
        self,
        text: str,
        top_k: int = 5,
        min_confidence: float = 0.1,
    ) -> List[Dict[str, Any]]:
        """Return top governance category matches for the provided context."""
        try:
            cleaned = self._preprocess_text_local(text or "")
            if not cleaned:
                return []

            # Ensure embeddings and centroids are ready
            if self._embedder is None or not self._category_centroids:
                self._init_local_embeddings()
            if self._embedder is None or not self._category_centroids:
                return []

            cache_key = f"gov_sem_matches::{hashlib.sha1(cleaned.encode('utf-8')).hexdigest()}::{top_k}::{min_confidence}"
            try:
                if hasattr(self, "_cache") and cache_key in self._cache:
                    cached = self._cache.get(cache_key)
                    if isinstance(cached, list):
                        return cached
            except Exception:
                pass

            scores = self._semantic_scores(cleaned)
            if not scores:
                return []

            ordered = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)
            matches: List[Dict[str, Any]] = []
            for category, score in ordered:
                if score < min_confidence:
                    continue
                matches.append({"category": category, "confidence": float(score)})
                if len(matches) >= top_k:
                    break

            try:
                if hasattr(self, "_cache"):
                    self._cache[cache_key] = list(matches)
            except Exception:
                pass

            return matches
        except Exception as e:
            logger.debug(f"_get_semantic_matches_gov failed: {e}", exc_info=True)
            return []

    def _keyword_scores(self, text: str) -> Dict[str, float]:
        """Lightweight keyword/regex fallback scores per category in [0,1]."""
        t = (text or '').lower()
        out: Dict[str, float] = {}
        try:
            for cat, toks in self._category_tokens.items():
                hits = 0
                for tok in toks:
                    try:
                        if re.search(r"\b" + re.escape(tok.lower()) + r"\b", t, re.IGNORECASE):
                            hits += 1
                    except Exception:
                        if tok.lower() in t:
                            hits += 1
                out[cat] = max(0.0, min(1.0, math.log1p(hits) / math.log1p(10)))
        except Exception:
            return {}
        return out

    def _pattern_scores(self, text: str) -> Dict[str, float]:
        """Compute governance pattern-based scores per category in [0,1]."""
        out: Dict[str, float] = {}
        try:
            if not hasattr(self, "_category_patterns") or not self._category_patterns:
                return out
            t = str(text or "")
            tl = t.lower()
            for cat, patterns in self._category_patterns.items():
                hits = 0
                for p in (patterns or [])[:20]:
                    try:
                        if not p:
                            continue
                        if re.search(p, t, re.IGNORECASE):
                            hits += 1
                    except Exception:
                        try:
                            if str(p).lower() in tl:
                                hits += 1
                        except Exception:
                            continue
                out[cat] = max(0.0, min(1.0, hits / 3.0))
        except Exception:
            return {}
        return out
    
    def _fallback_keyword_matching(self, text: str) -> Dict[str, float]:
        """Fallback keyword matching when embeddings/governance tables are unavailable."""
        scores: Dict[str, float] = {}
        t = (text or '').lower()
        
        # Hardcoded fallback keywords for common sensitive categories
        fallback_keywords = {
            'PII': ['customer', 'email', 'phone', 'mobile', 'ssn', 'passport', 'dob', 'date of birth', 
                   'first name', 'last name', 'address', 'contact', 'person', 'individual', 'name'],
            'Financial': ['account', 'bank', 'credit', 'debit', 'payment', 'invoice', 'salary', 'payroll',
                         'transaction', 'ledger', 'revenue', 'expense', 'balance', 'card number'],
            'Regulatory': ['gdpr', 'ccpa', 'hipaa', 'pci', 'compliance', 'regulation', 'consent', 'audit'],
            'Internal': ['internal', 'operational', 'config', 'setting', 'reference', 'lookup', 'status'],
        }
        
        for category, keywords in fallback_keywords.items():
            hits = 0
            for keyword in keywords:
                try:
                    if re.search(r'\b' + re.escape(keyword.lower()) + r'\b', t, re.IGNORECASE):
                        hits += 1
                except Exception:
                    if keyword.lower() in t:
                        hits += 1
            
            if hits > 0:
                # Score based on number of hits: 1 hit = 0.4, 2+ hits = 0.7+
                score = min(1.0, 0.3 + (hits * 0.2))
                scores[category] = score
        
        logger.info(f"    Fallback keyword matching for '{text[:60]}': {scores}")
        return scores

    def _load_patterns_from_governance(self, schema_fqn: str, category_name: str) -> List[str]:
        rows = []
        try:
            rows = snowflake_connector.execute_query(
                f"""
                SELECT COALESCE(pattern, PATTERN, PATTERN_STRING) AS PTN
                FROM {schema_fqn}.SENSITIVE_PATTERNS
                WHERE COALESCE(is_active, true) AND (
                    LOWER(category) = LOWER(%(n)s) OR LOWER(category_name) = LOWER(%(n)s)
                )
                """,
                {"n": category_name},
            ) or []
        except Exception:
            try:
                # CATEGORY_ID-based lookup using SENSITIVITY_CATEGORIES
                rows = snowflake_connector.execute_query(
                    f"""
                    SELECT COALESCE(pattern, PATTERN, PATTERN_STRING) AS PTN
                    FROM {schema_fqn}.SENSITIVE_PATTERNS
                    WHERE COALESCE(is_active, true) AND category_id IN (
                        SELECT CATEGORY_ID FROM {schema_fqn}.SENSITIVITY_CATEGORIES WHERE LOWER(CATEGORY_NAME) = LOWER(%(n)s)
                    )
                    """,
                    {"n": category_name},
                ) or []
            except Exception:
                try:
                    rows = snowflake_connector.execute_query(
                        f"""
                        SELECT COALESCE(pattern, PATTERN, PATTERN_STRING) AS PTN
                        FROM {schema_fqn}.SENSITIVITY_PATTERNS
                        WHERE COALESCE(is_active, true) AND category_id IN (
                            SELECT CATEGORY_ID FROM {schema_fqn}.SENSITIVITY_CATEGORIES WHERE LOWER(CATEGORY_NAME) = LOWER(%(n)s)
                        )
                        """,
                        {"n": category_name},
                    ) or []
                except Exception:
                    rows = []
        out: List[str] = []
        for r in rows:
            try:
                p = str(r.get("PTN") or "").strip()
                if p:
                    out.append(p)
            except Exception:
                continue
        # minimal fallbacks when nothing configured
        if not out:
            base = (category_name or "").strip().lower()
            if base in {"personal_data", "personal", "pii"}:
                out = [r"\bssn\b", r"\bemail\b", r"\bphone\b"]
            elif base in {"financial_data", "financial"}:
                out = [r"\binvoice\b", r"\bbank\b", r"\bpayment\b"]
            elif base in {"regulatory_data", "regulatory"}:
                out = [r"\bgdpr\b", r"\bhipaa\b", r"\bpci\b"]
        return out

    def _gov_semantic_scores(self, text: str) -> Dict[str, float]:
        out: Dict[str, float] = {}
        try:
            # Cache governance semantic matches
            cache_key = f"gov_sim::{text}"
            if hasattr(self, "_cache") and cache_key in self._cache:
                val = self._cache.get(cache_key) or {}
                if isinstance(val, dict):
                    return val
            svc = ai_classification_service
            # If governance match function or connector isn't available, skip
            if not hasattr(svc, "_get_semantic_matches_gov"):
                return out
            try:
                # Some environments may not have Snowflake or proper tables; trap and continue
                matches = svc._get_semantic_matches_gov(text) or []
            except Exception:
                matches = []
            try:
                from src.services.ai_assistant_service import ai_assistant_service as _aas
                _map = getattr(_aas, "_SEMANTIC_TO_AVENDRA", {}) or {}
            except Exception:
                _map = {}
            for m in matches:
                try:
                    # Tolerate different field names from governance queries
                    c = str(
                        m.get("category")
                        or m.get("CATEGORY")
                        or m.get("CATEGORY_NAME")
                        or ""
                    ).strip()
                    conf = float(m.get("confidence") or m.get("CONFIDENCE") or 0.0)
                except Exception:
                    continue
                if not c:
                    continue
                av = _map.get(c, c)
                out[av] = max(0.0, min(1.0, conf))
            try:
                if hasattr(self, "_cache"):
                    self._cache[cache_key] = dict(out)
            except Exception:
                pass
        except Exception:
            return {}
        return out

    def _calibrate_scores(self, scores: Dict[str, float]) -> Dict[str, float]:
        try:
            if not scores:
                return scores
            vals = list(scores.values())
            mn = min(vals)
            mx = max(vals)
            if mx <= 0:
                return dict(scores)
            if mx == mn:
                return dict(scores)
            gamma = 0.9 if mx < 0.5 else 1.1
            denom = float(mx - mn) or 1.0
            target_mx = max(mx, 0.5)
            out: Dict[str, float] = {}
            for k, v in scores.items():
                z = max(0.0, min(1.0, (float(v) - float(mn)) / denom))
                z2 = pow(z, gamma)
                out[k] = max(0.0, min(1.0, float(target_mx) * z2))
            return out
        except Exception:
            return dict(scores)

    def _apply_quality_calibration(self, scores: Dict[str, float], quality: Dict[str, float]) -> Dict[str, float]:
        """Adjust scores based on context quality metrics to calibrate confidence.

        Boost numeric-heavy contexts (SSN, credit cards, account numbers) and alpha-heavy contexts.
        Penalize only extremely short contexts.
        """
        try:
            if not scores:
                return scores
            qlen = float(quality.get("len", 0.0) or 0.0)
            dr = float(quality.get("digit_ratio", 0.0) or 0.0)
            ar = float(quality.get("alpha_ratio", 0.0) or 0.0)
            # base factor from length (log-like growth)
            base = 0.6 + min(0.4, (qlen / max(200.0, qlen + 1.0)) * 0.6)
            # boost numeric content (SSN, credit cards, account numbers are high-confidence PII)
            # high digit ratio (>0.5) indicates structured numeric PII: boost by up to 20%
            digit_boost = (1.0 + min(0.2, dr * 0.4))
            # boost alpha-heavy content (descriptive text)
            alpha_boost = (0.9 + min(0.25, ar * 0.25))
            factor = base * digit_boost * alpha_boost
            out: Dict[str, float] = {}
            for k, v in scores.items():
                out[k] = max(0.0, min(1.0, float(v) * float(factor)))
            return out
        except Exception:
            return scores

    def _auto_tune_parameters(self) -> None:
        try:
            valid_centroids = 0
            try:
                cents = getattr(self, "_category_centroids", {}) or {}
                valid_centroids = len([v for v in cents.values() if v is not None])
            except Exception:
                valid_centroids = 0
            has_sem = (self._embedder is not None) and valid_centroids > 0
            sem_ok = bool(getattr(self, "_embed_ready", False)) and (valid_centroids >= 3)
            
            logger.info(f"Auto-tuning parameters: embedder={self._embedder is not None}, embed_ready={self._embed_ready}, valid_centroids={valid_centroids}, sem_ok={sem_ok}")
            
            # Respect external overrides if provided via dynamic config
            try:
                cfg_over = None
                if hasattr(st, "session_state"):
                    cfg_over = st.session_state.get("ai_pipeline_config")
                if not cfg_over and settings is not None:
                    cfg_over = getattr(settings, "AI_PIPELINE_CONFIG", None)
                if isinstance(cfg_over, dict) and ("w_sem" in cfg_over or "w_kw" in cfg_over):
                    self._w_sem = float(cfg_over.get("w_sem", self._w_sem))
                    self._w_kw = float(cfg_over.get("w_kw", self._w_kw))
                    logger.info(f"  Using external config override: w_sem={self._w_sem}, w_kw={self._w_kw}")
                else:
                    if not sem_ok:
                        self._w_sem = 0.0
                        self._w_kw = 1.0
                        logger.info(f"  Regime: NO_EMBEDDINGS â†’ w_sem=0.0, w_kw=1.0 (keyword-only)")
                    elif valid_centroids < 6:
                        self._w_sem = 0.7
                        self._w_kw = 0.3
                        logger.info(f"  Regime: BALANCED ({valid_centroids} centroids) â†’ w_sem=0.7, w_kw=0.3")
                    else:
                        self._w_sem = 0.8
                        self._w_kw = 0.2
                        logger.info(f"  Regime: SEMANTIC_PREFERRED ({valid_centroids} centroids) â†’ w_sem=0.8, w_kw=0.2")
            except Exception as _e:
                logger.warning(f"  Exception during config override: {_e}")
                if not sem_ok:
                    self._w_sem = 0.0
                    self._w_kw = 1.0
                    logger.info(f"  Fallback: w_sem=0.0, w_kw=1.0 (keyword-only)")
                elif valid_centroids < 6:
                    self._w_sem = 0.7
                    self._w_kw = 0.3
                    logger.info(f"  Fallback: w_sem=0.7, w_kw=0.3 (balanced)")
                else:
                    self._w_sem = 0.8
                    self._w_kw = 0.2
                    logger.info(f"  Fallback: w_sem=0.8, w_kw=0.2 (semantic-preferred)")
            self._ctx_max_cols = max(int(self._ctx_max_cols or 0), 20)
            self._ctx_max_vals = max(int(self._ctx_max_vals or 0), 7)
            self._col_sample_rows = max(int(self._col_sample_rows or 0), 400)
            try:
                thr = float(self._conf_label_threshold if self._conf_label_threshold is not None else 0.30)
            except Exception:
                thr = 0.30
            self._conf_label_threshold = max(0.0, min(1.0, thr))
            self._debug = False
        except Exception:
            pass

    def _load_dynamic_config(self) -> Dict[str, Any]:
        """Load dynamic configuration for the pipeline with sensible defaults and layered overrides.

        Precedence (lowest to highest):
        defaults < settings.AI_PIPELINE_CONFIG < st.secrets['AI_PIPELINE_CONFIG'] < st.session_state['ai_pipeline_config']
        """
        cfg: Dict[str, Any] = {
            "enable_context_enhancement": True,
            "min_context_length": 200,
            "adaptive_sampling": True,
            "apply_confidence_calibration": True,
            # expose weights so they can be tuned without code changes
            "w_sem": float(getattr(self, "_w_sem", 0.85) or 0.85),
            "w_kw": float(getattr(self, "_w_kw", 0.15) or 0.15),
        }

        # Merge from app settings if available
        try:
            if settings is not None:
                base = getattr(settings, "AI_PIPELINE_CONFIG", None)
                if isinstance(base, dict):
                    cfg.update(base)
        except Exception:
            pass

        # Merge from Streamlit secrets if available
        try:
            if hasattr(st, "secrets"):
                sec = st.secrets.get("AI_PIPELINE_CONFIG")  # type: ignore[attr-defined]
                if isinstance(sec, dict):
                    cfg.update(sec)  # type: ignore[arg-type]
        except Exception:
            pass

        # Merge from session state for on-the-fly adjustments
        try:
            if hasattr(st, "session_state"):
                ses = st.session_state.get("ai_pipeline_config")
                if isinstance(ses, dict):
                    cfg.update(ses)
        except Exception:
            pass

        # Sanitize/validate types and ranges
        try:
            mcl = int(cfg.get("min_context_length", 200) or 200)
            cfg["min_context_length"] = max(0, mcl)
        except Exception:
            cfg["min_context_length"] = 200
        try:
            ws = float(cfg.get("w_sem", 0.85) or 0.85)
            wk = float(cfg.get("w_kw", 0.15) or 0.15)
            cfg["w_sem"] = max(0.0, min(1.0, ws))
            cfg["w_kw"] = max(0.0, min(1.0, wk))
        except Exception:
            cfg["w_sem"] = 0.85
            cfg["w_kw"] = 0.15
        try:
            for flag in ("enable_context_enhancement", "adaptive_sampling", "apply_confidence_calibration"):
                cfg[flag] = bool(cfg.get(flag, cfg.get(flag, True)))
        except Exception:
            pass

        return cfg

    def _preprocess_text_local(self, text: str, remove_stopwords: bool = False) -> str:
        s = str(text or "")
        try:
            s = re.sub(r"[\n\r\t]+", " ", s)
            s = re.sub(r"\s+", " ", s).strip()
        except Exception:
            pass
        if not remove_stopwords:
            return s
        # Conservative stopword filter: preserve domain-critical terms
        # Removed: "data", "info", "information" (essential for PII/classification context)
        # Removed: "in", "for" (common in data patterns like "in database", "for classification")
        stops = {"the","a","an","and","or","of","to","on","at","by","with","from","as","is","are","was","were"}
        try:
            toks = [w for w in re.split(r"[^a-zA-Z0-9]+", s) if w and w.lower() not in stops]
            return " ".join(toks)
        except Exception:
            return s

    def _normalize_category_for_cia(self, category: Optional[str]) -> str:
        """Map a detected category to the canonical Avendra category for CIA mapping.

        Handles internal labels like 'PII', 'Financial', 'Regulatory', 'Internal' by
        mapping them through ai_assistant_service._SEMANTIC_TO_AVENDRA when possible.
        """
        try:
            raw = (category or '').strip()
            if not raw:
                return ''
            try:
                sem_map = getattr(self.ai_service, "_SEMANTIC_TO_AVENDRA", {}) or {}
            except Exception:
                sem_map = {}
            mapped = sem_map.get(raw, raw)
            return mapped
        except Exception:
            return str(category or '').strip()

    def _context_quality_metrics(self, text: str) -> Dict[str, float]:
        s = str(text or "")
        n = len(s)
        if n <= 0:
            return {"len": 0, "digit_ratio": 1.0, "alpha_ratio": 0.0, "too_short": True}
        digits = sum(1 for ch in s if ch.isdigit())
        alpha = sum(1 for ch in s if ch.isalpha())
        # configurable minimum context length
        try:
            mcl = int(self._load_dynamic_config().get("min_context_length", 200))
        except Exception:
            mcl = 200
        return {
            "len": n,
            "digit_ratio": float(digits) / float(max(1, n)),
            "alpha_ratio": float(alpha) / float(max(1, n)),
            "too_short": n < mcl,
        }

    def _detect_sensitive_columns_local(self, database: str, schema: str, table: str, max_cols: int = 200) -> List[Dict[str, Any]]:
        rows: List[Dict[str, Any]] = []
        try:
            cols = snowflake_connector.execute_query(
                f"""
                SELECT COLUMN_NAME, DATA_TYPE, COMMENT
                FROM {database}.INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_SCHEMA = %(s)s AND TABLE_NAME = %(t)s
                ORDER BY ORDINAL_POSITION
                LIMIT {max_cols}
                """,
                {"s": schema, "t": table},
            ) or []
        except Exception:
            cols = []
        for c in cols:
            try:
                cn = str(c.get("COLUMN_NAME") or "")
                dt = str(c.get("DATA_TYPE") or "")
                com = str(c.get("COMMENT") or "")
                samples = self._sample_column_values(
                    database,
                    schema,
                    table,
                    cn,
                    sample_rows=int(getattr(self, "_col_sample_rows", 60) or 60),
                ) or []
                filtered_vals = []
                for v in samples:
                    sv = str(v)
                    if not sv.strip():
                        continue
                    if sv.strip().isdigit() and len(sv.strip()) < 5:
                        continue
                    filtered_vals.append(sv[:64])
                    if len(filtered_vals) >= 8:
                        break
                sample_txt = ", ".join(filtered_vals[:8])
                ctx_parts = [f"{database}.{schema}.{table}.{cn}"]
                if com:
                    ctx_parts.append(com)
                if sample_txt:
                    ctx_parts.append(sample_txt)
                ctx = " | ".join([p for p in ctx_parts if p])

                ptxt = self._preprocess_text_local(ctx)
                sem = self._semantic_scores(ptxt)
                kw = self._keyword_scores(ptxt)
                comb: Dict[str, float] = {}
                keys = set(list(sem.keys()) + list(kw.keys()))
                w_sem = float(getattr(self, "_w_sem", 0.7))
                w_kw = float(getattr(self, "_w_kw", 0.3))
                for k in keys:
                    s = float(sem.get(k, 0.0))
                    k2 = float(kw.get(k, 0.0))
                    comb[k] = max(0.0, min(1.0, (w_sem * s) + (w_kw * k2)))
                if not comb:
                    best = None
                    conf = 0.0
                else:
                    best = max(comb, key=comb.get)
                    conf = float(comb.get(best, 0.0))

                try:
                    canon_cat = self._normalize_category_for_cia(best)
                    c_level, i_level, a_level = ai_assistant_service.CIA_MAPPING.get(canon_cat or '', (1, 1, 1))
                except Exception:
                    c_level, i_level, a_level = (1, 1, 1)
                try:
                    label = ai_assistant_service._map_cia_to_label(c_level, i_level, a_level)
                except Exception:
                    label = "Internal"
                rows.append({
                    "Column": cn,
                    "Type": dt,
                    "Sensitivity": label,
                    "Confidence": round(conf, 4),
                    "Categories": best or "",
                    "C": int(c_level),
                    "I": int(i_level),
                    "A": int(a_level),
                })
            except Exception:
                continue
        return rows

    def _classify_assets_local(self, db: str, assets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Classify assets using local MiniLM + keyword fallback and produce results compatible with UI."""
        results: List[Dict[str, Any]] = []
        for asset in assets:
            try:
                # Derive business context using metadata and samples (no governance glossary)
                context = ""
                try:
                    context = self.ai_service._derive_business_context(asset)  # type: ignore
                except Exception:
                    context = f"Table {asset.get('table')} in schema {asset.get('schema')}"
                # Build richer context with targeted samples and column names
                rich_ctx = self._build_richer_context(
                    asset,
                    max_cols=int(getattr(self, "_ctx_max_cols", 8) or 8),
                    max_vals=int(getattr(self, "_ctx_max_vals", 3) or 3),
                    business_purpose=context,
                )
                full_context = "\n".join([c for c in [context, rich_ctx] if c])

                q = self._context_quality_metrics(full_context)
                if q.get("too_short") or q.get("digit_ratio", 0.0) > 0.6:
                    try:
                        rich_ctx2 = self._build_richer_context(
                            asset,
                            max_cols=int(max(20, int(getattr(self, "_ctx_max_cols", 8)) * 2)),
                            max_vals=int(max(7, int(getattr(self, "_ctx_max_vals", 3)) * 2)),
                            business_purpose=context,
                        )
                        full_context = "\n".join([c for c in [context, rich_ctx2] if c])
                    except Exception:
                        pass

                ptxt = self._preprocess_text_local(full_context)
                sem = self._semantic_scores(ptxt)
                kw = self._keyword_scores(ptxt)
                pt = self._pattern_scores(ptxt)
                combined: Dict[str, float] = {}
                cats = set(list(sem.keys()) + list(kw.keys()) + list(pt.keys()))

                # Diagnostic logging for score computation
                logger.info(f"Score computation for {asset}:")
                logger.info(f"  Semantic scores: {sem}")
                logger.info(f"  Keyword scores: {kw}")
                logger.info(f"  Pattern scores: {pt}")

                # weights: allow dynamic override
                cfg = {}
                try:
                    cfg = self._load_dynamic_config()
                except Exception:
                    cfg = {}
                w_sem = float(cfg.get("w_sem", getattr(self, "_w_sem", 0.7)))
                w_kw = float(cfg.get("w_kw", getattr(self, "_w_kw", 0.3)))
                w_pt = float(cfg.get("w_pt", 0.2))
                # normalize weights so they sum <= 1; if exceed, rescale
                try:
                    total_w = w_sem + w_kw + w_pt
                    if total_w > 1.0:
                        w_sem /= total_w
                        w_kw /= total_w
                        w_pt /= total_w
                except Exception:
                    pass

                logger.info(f"  Weights: w_sem={w_sem:.2f}, w_kw={w_kw:.2f}, w_pt={w_pt:.2f}")

                for cat in cats:
                    s = float(sem.get(cat, 0.0))
                    k = float(kw.get(cat, 0.0))
                    p = float(pt.get(cat, 0.0))
                    v = (w_sem * s) + (w_kw * k) + (w_pt * p)
                    combined[cat] = max(0.0, min(1.0, v))
                    logger.debug(f"    {cat}: sem={s:.3f}, kw={k:.3f}, pt={p:.3f} â†’ combined={combined[cat]:.3f}")
                try:
                    g = self._gov_semantic_scores(ptxt)
                    if g:
                        w_gov = 0.25
                        keys = set(list(combined.keys()) + list(g.keys()))
                        for k in keys:
                            base = float(combined.get(k, 0.0))
                            gv = float(g.get(k, 0.0))
                            combined[k] = max(0.0, min(1.0, (1.0 - w_gov) * base + w_gov * gv))
                except Exception:
                    pass
                # Apply heuristic boosts for FINANCIAL/REGULATORY from structure/content
                boosts = self._category_boosts(asset, full_context)
                if boosts:
                    for cat, b in boosts.items():
                        combined[cat] = max(0.0, min(1.0, combined.get(cat, 0.0) + b))
                combined = self._apply_quality_calibration(self._calibrate_scores(combined), q)
                if not combined:
                    best_cat = None
                    confidence = 0.0
                else:
                    best_cat = max(combined, key=combined.get)
                    confidence = float(combined.get(best_cat, 0.0))

                # Governance alignment boost
                try:
                    gov_hint = None
                    if isinstance(g, dict) and g:
                        gov_hint = max(g.items(), key=lambda x: x[1])[0]
                    if gov_hint and gov_hint == best_cat:
                        confidence = min(1.0, confidence * 1.15)
                except Exception:
                    pass

                if bool(getattr(self, "_debug", False)):
                    try:
                        top = sorted(combined.items(), key=lambda x: x[1], reverse=True)[:5]
                        st.caption("Debug: top category scores (normalized)")
                        st.write(top)
                        st.caption("Debug: context sample")
                        st.code((full_context or "")[:800])
                    except Exception:
                        pass

                # CIA mapping and label
                try:
                    canon_cat = self._normalize_category_for_cia(best_cat)
                    c, i, a = self.ai_service.CIA_MAPPING.get(canon_cat, (1, 1, 1))  # type: ignore
                except Exception:
                    c, i, a = (1, 1, 1)
                try:
                    score = int(max(int(c), int(i), int(a)))
                except Exception:
                    score = 1
                if score <= 0:
                    base_label = 'Public'
                elif score == 1:
                    base_label = 'Internal'
                elif score == 2:
                    base_label = 'Confidential'
                else:
                    base_label = 'Restricted'
                lbl_thr = float(getattr(self, "_conf_label_threshold", 0.45))
                final_label = base_label if confidence >= lbl_thr else "Uncertain â€” review"

                # Validation & routing (disabled to avoid database errors)
                validation_status = 'REVIEW_REQUIRED'
                issues = ['Validation disabled']
                route = 'STANDARD_REVIEW'

                # SQL preview for tagging (disabled to avoid database errors)
                sql_preview = None

                # Emoji label for UI and compliance hints
                label_emoji_map = {
                    'Confidential': 'ðŸŸ¥ Confidential',
                    'Restricted': 'ðŸŸ§ Restricted',
                    'Internal': 'ðŸŸ¨ Internal',
                    'Public': 'ðŸŸ© Public',
                    'Uncertain â€” review': 'â¬œ Uncertain â€” review',
                }
                color_map = {
                    'Confidential': 'Red',
                    'Restricted': 'Orange',
                    'Internal': 'Yellow',
                    'Public': 'Green',
                    'Uncertain â€” review': 'Gray',
                }
                label_emoji = label_emoji_map.get(final_label, final_label)
                # Prefer governance-derived compliance mapping
                compliance = []
                try:
                    from src.services.ai_classification_service import ai_classification_service as _svc
                    comp_map = getattr(_svc, "_gov_cat_compliance", {}) or {}
                    try:
                        sem_to_av = getattr(ai_assistant_service, "_SEMANTIC_TO_AVENDRA", {}) or {}
                    except Exception:
                        sem_to_av = {}
                    if best_cat and comp_map:
                        # Gather governance categories that map to the detected category
                        mapped = [gcat for gcat, av in sem_to_av.items() if av == best_cat]
                        tags = []
                        for gcat in (mapped or []):
                            try:
                                tags.extend(comp_map.get(gcat, []) or [])
                            except Exception:
                                continue
                        if not tags:
                            tags = comp_map.get(best_cat, []) or []
                        compliance = list(dict.fromkeys(tags))
                except Exception:
                    compliance = ai_assistant_service._COMPLIANCE_MAP.get(best_cat, []) if hasattr(ai_assistant_service, '_COMPLIANCE_MAP') else []

                # Confidence normalization and tiering (non-breaking: keep confidence in 0..1)
                confidence_pct = round(confidence * 100.0, 1)
                if confidence_pct < 50.0:
                    confidence_tier = "Uncertain"
                elif confidence_pct < 75.0:
                    confidence_tier = "Likely"
                else:
                    confidence_tier = "Confident"

                results.append({
                    'asset': asset,
                    'business_context': full_context,
                    'category': best_cat,
                    'confidence': confidence,
                    'confidence_pct': confidence_pct,
                    'confidence_tier': confidence_tier,
                    'c': c,
                    'i': i,
                    'a': a,
                    'label': final_label,
                    'label_emoji': label_emoji,
                    'color': color_map.get(final_label, ''),
                    'validation_status': validation_status,
                    'issues': issues,
                    'route': route,
                    'application_status': 'QUEUED_FOR_REVIEW',
                    'failure_reason': None,
                    'sql_preview': sql_preview,
                    'compliance': compliance,
                })
            except Exception as e:
                results.append({'asset': asset, 'error': str(e)})
        return results

    def _classify_columns_local(self, db: str, schema: str, table: str, max_cols: int = 50) -> List[Dict[str, Any]]:
        """Classify individual columns using MiniLM embeddings + governance tables for high-confidence detection."""
        results: List[Dict[str, Any]] = []
        try:
            # Ensure embeddings are initialized
            if self._embedder is None or not self._category_centroids:
                logger.info("Initializing embeddings for column classification...")
                self._init_local_embeddings()
                logger.info(f"Embeddings initialized: {len(self._category_centroids)} centroids")

            # Fetch columns from information_schema
            cols = snowflake_connector.execute_query(
                f"""
                SELECT COLUMN_NAME, DATA_TYPE, COMMENT
                FROM {db}.INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_SCHEMA = %(s)s AND TABLE_NAME = %(t)s
                ORDER BY ORDINAL_POSITION
                LIMIT {max_cols}
                """,
                {"s": schema, "t": table},
            ) or []

            logger.info(f"Column-level classification: {db}.{schema}.{table} with {len(cols)} columns")

            if not cols:
                logger.warning(f"No columns found for {db}.{schema}.{table}")
                return results

            for col in cols:
                try:
                    col_name = str(col.get('COLUMN_NAME') or '')
                    col_type = str(col.get('DATA_TYPE') or '')
                    col_comment = str(col.get('COMMENT') or '')

                    if not col_name:
                        continue

                    # Build column context: name + type + comment + sample values
                    col_context_parts = [col_name, col_type]
                    if col_comment:
                        col_context_parts.append(col_comment)

                    # Sample column values for context
                    try:
                        samples = self._sample_column_values(db, schema, table, col_name, sample_rows=20) or []
                        if samples:
                            sample_str = ", ".join([str(s)[:32] for s in samples[:5]])
                            col_context_parts.append(f"Examples: {sample_str}")
                    except Exception:
                        pass

                    col_context = " | ".join(col_context_parts)

                    # Compute scores using embeddings + keywords + patterns
                    ptxt = self._preprocess_text_local(col_context)

                    # Diagnostic logging
                    logger.info(f"  {col_name} context: {col_context[:80]}")
                    logger.info(f"    Preprocessed: {ptxt[:80]}")
                    logger.info(f"    Embedder ready: {self._embedder is not None}, Centroids: {len(self._category_centroids) if self._category_centroids else 0}")

                    sem = self._semantic_scores(ptxt)
                    kw = self._keyword_scores(ptxt)
                    pt = self._pattern_scores(ptxt)

                    # Log embedding scores for debugging
                    logger.info(f"    Semantic scores: {sem}")
                    logger.info(f"    Keyword scores: {kw}")
                    logger.info(f"    Pattern scores: {pt}")

                    # Combine scores with governance-aware weights
                    combined: Dict[str, float] = {}
                    cats = set(list(sem.keys()) + list(kw.keys()) + list(pt.keys()))

                    # Use higher semantic weight for column-level detection (embeddings are more accurate for short text)
                    w_sem = 0.75
                    w_kw = 0.20
                    w_pt = 0.15

                    for cat in cats:
                        s = float(sem.get(cat, 0.0))
                        k = float(kw.get(cat, 0.0))
                        p = float(pt.get(cat, 0.0))
                        v = (w_sem * s) + (w_kw * k) + (w_pt * p)
                        combined[cat] = max(0.0, min(1.0, v))

                    # FALLBACK: If no scores from embeddings/keywords/patterns, use simple keyword matching
                    if not combined:
                        logger.info(f"    No scores from embeddings/keywords/patterns, using fallback matching")
                        fallback_scores = self._fallback_keyword_matching(col_context)
                        combined = fallback_scores
                        logger.info(f"    Fallback scores: {combined}")

                    logger.debug(f"    Combined scores: {combined}")

                    # Apply governance table boost if available
                    try:
                        gov_scores = self._gov_semantic_scores(ptxt)
                        logger.debug(f"    Governance scores: {gov_scores}")
                        if gov_scores:
                            for cat in gov_scores:
                                gov_val = float(gov_scores.get(cat, 0.0))
                                base_val = float(combined.get(cat, 0.0))
                                # Governance tables provide strong signal: 30% weight
                                combined[cat] = max(0.0, min(1.0, 0.7 * base_val + 0.3 * gov_val))
                            logger.debug(f"    After gov boost: {combined}")
                    except Exception as e:
                        logger.debug(f"    Gov scores error: {e}")

                    # Get best category and confidence
                    if combined:
                        best_cat = max(combined, key=combined.get)
                        confidence = float(combined.get(best_cat, 0.0))
                    else:
                        best_cat = None
                        confidence = 0.0

                    logger.debug(f"    Before calibration - best_cat: {best_cat}, confidence: {confidence:.3f}")

                    # Apply quality calibration
                    q = self._context_quality_metrics(col_context)
                    logger.debug(f"    Quality metrics: {q}")
                    if combined:
                        combined = self._apply_quality_calibration(self._calibrate_scores(combined), q)
                        if combined:
                            best_cat = max(combined, key=combined.get)
                            confidence = float(combined.get(best_cat, 0.0))
                            logger.debug(f"    After calibration - best_cat: {best_cat}, confidence: {confidence:.3f}")

                    # Determine label based on confidence
                    try:
                        canon_cat = self._normalize_category_for_cia(best_cat)
                        c, i, a = self.ai_service.CIA_MAPPING.get(canon_cat, (1, 1, 1))  # type: ignore
                    except Exception:
                        c, i, a = (1, 1, 1)

                    # Derive a display category strictly limited to PII, SOX, SOC2
                    display_cat = None
                    try:
                        legacy_map = {
                            'PERSONAL_DATA': 'PII',
                            'PERSONAL DATA': 'PII',
                            'PERSONAL': 'PII',
                            'PII': 'PII',
                            'PERSONAL FINANCIAL DATA': 'SOX',
                            'FINANCIAL_DATA': 'SOX',
                            'FINANCIAL DATA': 'SOX',
                            'FINANCIAL': 'SOX',
                            'PROPRIETARY_DATA': 'SOX',
                            'PROPRIETARY DATA': 'SOX',
                            'REGULATORY_DATA': 'SOC2',
                            'REGULATORY DATA': 'SOC2',
                            'REGULATORY': 'SOC2',
                            'INTERNAL_DATA': 'SOC2',
                            'INTERNAL DATA': 'SOC2',
                            'SOC2': 'SOC2',
                            'SOX': 'SOX',
                        }
                        if best_cat:
                            key = str(best_cat).strip().upper()
                            normalized = legacy_map.get(key)
                            if normalized in {'PII', 'SOX', 'SOC2'}:
                                display_cat = normalized
                    except Exception:
                        display_cat = None
                    
                    # Assign label based on confidence and CIA levels
                    # Lower threshold to 0.20 to catch more sensitive columns
                    if confidence >= 0.20:
                        if c >= 2 or i >= 2 or a >= 2:
                            label = 'Restricted' if max(c, i, a) >= 3 else 'Confidential'
                        else:
                            label = 'Internal'
                    else:
                        label = 'Uncertain'
                    
                    logger.info(f"  Column {col_name}: {display_cat or best_cat} (raw={best_cat}) @ {confidence:.1%} â†’ {label}")
                    
                    # Include ALL columns with a detected category and minimum confidence
                    # Don't filter out 'Uncertain' here - let display layer decide what to show
                    if best_cat and confidence >= 0.15:
                        logger.info(f"    âœ“ INCLUDED: {col_name} ({display_cat or best_cat}, {confidence:.1%})")
                        results.append({
                            'column': col_name,
                            'data_type': col_type,
                            'comment': col_comment,
                            'context': col_context,
                            'category': display_cat if display_cat else best_cat,
                            'confidence': confidence,
                            'confidence_pct': round(confidence * 100.0, 1),
                            'label': label,
                            'c': c,
                            'i': i,
                            'a': a,
                            'scores': combined,
                        })
                    else:
                        logger.info(f"    âœ— FILTERED OUT: {col_name} (best_cat={best_cat}, confidence={confidence:.3f})")
                except Exception as e:
                    logger.warning(f"Column classification failed for {col_name}: {e}", exc_info=True)
                    # Don't add error rows to results - just log and skip
        except Exception as e:
            logger.error(f"Column-level classification failed: {e}", exc_info=True)
        
        logger.info(f"âœ“ Column classification complete: {len(results)} columns included in results")
        return results

    def _build_richer_context(self, asset: Dict[str, Any], max_cols: int = 120, max_vals: int = 10, business_purpose: Optional[str] = None) -> str:
        """Aggregate business purpose, column names, and example values in a single contextual string."""
        table_name = str(asset.get('table') or '')
        schema_name = str(asset.get('schema') or '')
        bp = str(business_purpose or asset.get('comment') or '').strip()
        # Cache lookup
        try:
            ck = f"ctx::{asset.get('full_name','')}::{int(max_cols)}::{int(max_vals)}::{bp[:64]}"
            if hasattr(self, "_cache") and ck in self._cache:
                v = self._cache.get(ck)
                if isinstance(v, str):
                    return v
        except Exception:
            pass
        cols_list: List[str] = []
        sample_values: List[str] = []
        stats_map: Dict[str, Dict[str, Any]] = {}
        meta_map: Dict[str, Dict[str, Any]] = {}
        try:
            cols = snowflake_connector.execute_query(
                f"""
                SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE, CHARACTER_MAXIMUM_LENGTH, COMMENT
                FROM {asset['database']}.INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_SCHEMA = %(s)s AND TABLE_NAME = %(t)s
                ORDER BY ORDINAL_POSITION
                LIMIT {max_cols}
                """,
                {"s": asset.get('schema'), "t": asset.get('table')},
            ) or []
        except Exception:
            cols = []
        if cols:
            cols_list = [
                (f"{str(c.get('COLUMN_NAME'))} ({str(c.get('DATA_TYPE'))})")
                for c in cols if c.get('COLUMN_NAME')
            ]
            try:
                for c in cols:
                    cn = str(c.get('COLUMN_NAME') or '')
                    if not cn:
                        continue
                    meta_map[cn] = {
                        "type": str(c.get('DATA_TYPE') or ''),
                        "nullable": str(c.get('IS_NULLABLE') or '').upper() in ('YES','Y','TRUE','1'),
                        "length": c.get('CHARACTER_MAXIMUM_LENGTH'),
                        "comment": c.get('COMMENT')
                    }
            except Exception:
                meta_map = {}
        # Targeted sampling prioritizing columns matched by governance-configured tokens/regex
        try:
            # Load dynamic patterns from sensitivity config
            try:
                cfg = ai_classification_service.load_sensitivity_config() or {}
            except Exception:
                cfg = {}
            pats = cfg.get("patterns") or {}
            token_set = set()
            regex_list: List[str] = []
            try:
                for _cat, spec in pats.items():
                    try:
                        for t in (spec.get("name_tokens") or []):
                            if t:
                                token_set.add(str(t).lower())
                    except Exception:
                        pass
                    try:
                        for k in (spec.get("keywords") or []):
                            if k:
                                token_set.add(str(k).lower())
                    except Exception:
                        pass
                    try:
                        for rx in (spec.get("patterns") or spec.get("regex_list") or []):
                            if rx:
                                regex_list.append(str(rx))
                    except Exception:
                        pass
            except Exception:
                token_set = set(); regex_list = []
            # Fallback default PII tokens/patterns if governance config is empty
            if not token_set and not regex_list:
                token_set = {
                    "ssn","social","credit","debit","card","cvv","iban","swift","routing","account","acct","tax","tin","pan","aadhaar","passport","phone","mobile","email","mail","dob","birth","dateofbirth","address"
                }
                regex_list = [r"\b(?:ssn|iban|swift|tin|pan|aadhaar)\b", r"\b(?:credit|debit)\b", r"\b(?:phone|mobile)\b", r"\b(?:email)\b"]

            def _name_matches(cname: str) -> bool:
                cl = (cname or "").lower()
                try:
                    for tok in token_set:
                        if tok and tok in cl:
                            return True
                except Exception:
                    pass
                try:
                    for rx in regex_list:
                        try:
                            if re.search(rx, cname, re.IGNORECASE):
                                return True
                        except Exception:
                            continue
                except Exception:
                    pass
                return False

            # Choose columns matched by config; prioritize text/commented columns; fallback to first N for coverage
            selected_cols = []
            try:
                text_like = []
                others = []
                for c in cols[:max_cols]:
                    cn = str(c.get('COLUMN_NAME') or '')
                    dt = str(c.get('DATA_TYPE') or '').upper()
                    has_comment = bool(str(c.get('COMMENT') or '').strip())
                    is_text = any(x in dt for x in ["CHAR","TEXT","STRING","VARCHAR","VARIANT"])
                    if cn and _name_matches(cn):
                        selected_cols.append(c)
                    elif is_text or has_comment:
                        text_like.append(c)
                    else:
                        others.append(c)
                if len(selected_cols) < max_cols:
                    for c in text_like:
                        if len(selected_cols) >= max_cols:
                            break
                        if c not in selected_cols:
                            selected_cols.append(c)
                if len(selected_cols) < max_cols:
                    for c in others:
                        if len(selected_cols) >= max_cols:
                            break
                        if c not in selected_cols:
                            selected_cols.append(c)
            except Exception:
                selected_cols = cols[:max_cols]

            total_rows = None
            try:
                tr = snowflake_connector.execute_query(f"SELECT COUNT(*) AS N FROM {asset['full_name']}") or []
                if tr:
                    total_rows = int(list(tr[0].values())[0])
            except Exception:
                total_rows = None

            for c in selected_cols:
                cn = str(c.get('COLUMN_NAME') or '')
                if not cn:
                    continue
                try:
                    qstats = (
                        f"SELECT COUNT(*) AS TOTAL, "
                        f"COUNT_IF(\"{cn}\" IS NULL) AS NULLS, "
                        f"APPROX_COUNT_DISTINCT(\"{cn}\") AS DISTINCTS "
                        f"FROM {asset['full_name']}"
                    )
                    srows = snowflake_connector.execute_query(qstats) or []
                    if srows:
                        row0 = srows[0]
                        tot = int(row0.get('TOTAL') or 0)
                        nulls = int(row0.get('NULLS') or 0)
                        dcnt = int(row0.get('DISTINCTS') or 0)
                        base = tot if tot else (total_rows or 0)
                        null_ratio = float(nulls) / float(base) if base else 0.0
                        uniq_ratio = float(dcnt) / float(base) if base else 0.0
                        stats_map.setdefault(cn, {})
                        stats_map[cn].update({"null_ratio": null_ratio, "uniq_ratio": uniq_ratio})
                except Exception:
                    pass
                try:
                    dt = str((meta_map.get(cn, {}) or {}).get("type") or '').upper()
                    if any(x in dt for x in ["CHAR","TEXT","STRING","VARCHAR"]):
                        qlen = (
                            f"SELECT AVG(LENGTH(\"{cn}\")) AS AVGL "
                            f"FROM {asset['full_name']} SAMPLE ({max(max_vals*50, 100)} ROWS) "
                            f"WHERE \"{cn}\" IS NOT NULL"
                        )
                        lrows = snowflake_connector.execute_query(qlen) or []
                        if lrows:
                            avgl = float(list(lrows[0].values())[0] or 0.0)
                            stats_map.setdefault(cn, {})
                            stats_map[cn].update({"avg_len": avgl})
                except Exception:
                    pass
                try:
                    q = (
                        f"SELECT \"{cn}\" AS V FROM {asset['full_name']} SAMPLE ({max_vals*10} ROWS) "
                        f"WHERE \"{cn}\" IS NOT NULL"
                    )
                    rows = snowflake_connector.execute_query(q) or []
                    cnt = 0
                    for r in rows:
                        v = r.get('V')
                        if v is None:
                            continue
                        sv = str(v)
                        # Skip only pure integers (avoid skipping formatted numeric PII like SSN, cards)
                        if re.fullmatch(r"\s*\d+\s*", sv) and len(sv.strip()) < 5:
                            continue
                        sv_trim = sv[:200] + ("â€¦" if len(sv) > 200 else "")
                        sample_values.append(f"{cn}:{sv_trim}")
                        cnt += 1
                        if cnt >= max_vals:
                            break
                except Exception:
                    continue
        except Exception:
            pass
        # Build enriched column summary with stats
        col_items: List[str] = []
        try:
            for item in cols_list[:max_cols]:
                try:
                    nm, rest = item.split(" ", 1)
                except Exception:
                    nm, rest = item, ""
                m = meta_map.get(nm, {})
                stx = stats_map.get(nm, {})
                nr = stx.get("null_ratio", 0.0)
                ur = stx.get("uniq_ratio", 0.0)
                al = stx.get("avg_len")
                nullable = "Y" if m.get("nullable") else "N"
                length = m.get("length")
                cm = m.get("comment")
                frag = f"{nm} {rest} null={nr:.0%} unique={ur:.0%}"
                if al is not None:
                    try:
                        frag += f" avglen={float(al):.0f}"
                    except Exception:
                        pass
                if length:
                    frag += f" len={length}"
                frag += f" nullable={nullable}"
                if cm:
                    cmt = str(cm).strip().replace('\n',' ')[:80]
                    frag += f" comment={cmt}"
                col_items.append(frag)
        except Exception:
            col_items = cols_list[:max_cols]
        col_names = ", ".join(col_items) if col_items else ""
        # Include more examples overall with an upper cap
        max_examples = min(max_cols * max_vals, 600)
        examples = ", ".join(sample_values[: max_examples ]) if sample_values else ""
        # infer business domain from schema/table names
        inferred = None
        try:
            name_blob = f"{schema_name} {table_name} {bp}".lower()
            domain_map = {
                "hr": "Human Resources",
                "employee": "Human Resources",
                "payroll": "Human Resources",
                "fin": "Finance",
                "acct": "Finance",
                "gl": "Finance",
                "ap": "Finance",
                "ar": "Finance",
                "invoice": "Finance",
                "bank": "Finance",
                "crm": "Customer Relationship",
                "sales": "Sales",
                "marketing": "Marketing",
                "pii": "Personal Data",
                "customer": "Customer Data",
                "vendor": "Supplier/Vendor",
                "reg": "Regulatory",
            }
            for tok, dom in domain_map.items():
                if tok in name_blob:
                    inferred = dom
                    break
        except Exception:
            inferred = None

        parts = []
        parts.append(f"Table {table_name} in schema {schema_name}.")
        if bp:
            parts.append(f"Business purpose: {bp}.")
        if inferred:
            parts.append(f"Inferred domain: {inferred}.")
        if col_names:
            parts.append(f"Columns: {col_names}.")
        if examples:
            parts.append(f"Example data: {examples}.")
        ctx_out = " ".join(parts)
        try:
            if hasattr(self, "_cache"):
                self._cache[ck] = ctx_out
        except Exception:
            pass
        return ctx_out

    def _category_boosts(self, asset: Dict[str, Any], context: str) -> Dict[str, float]:
        """Dynamic pattern/keyword regex boosts using SENSITIVE_* configuration (no hardcoded patterns)."""
        boosts: Dict[str, float] = {}
        # Cache lookup
        try:
            ck = f"boosts::{asset.get('full_name','')}::{(context or '')[:256]}"
            if hasattr(self, "_cache") and ck in self._cache:
                v = self._cache.get(ck) or {}
                if isinstance(v, dict):
                    return v
        except Exception:
            pass
        # Build evaluation text from asset identifiers + context
        eval_text = " ".join([
            asset.get('full_name', ''),
            asset.get('schema', ''),
            asset.get('table', ''),
            context or ''
        ])
        # Load sensitivity configuration
        try:
            cfg = ai_classification_service.load_sensitivity_config() or {}
        except Exception:
            cfg = {}
        patterns = cfg.get('patterns') or {}
        # Detect configured pattern/keyword hits using the service helper
        try:
            if hasattr(ai_classification_service, "_detect_patterns_in_context"):
                raw = ai_classification_service._detect_patterns_in_context(eval_text, patterns)
            else:
                raw = {}
        except Exception:
            raw = {}
        # Map governance categories to Avendra categories when applicable
        try:
            sem_to_av = getattr(ai_assistant_service, "_SEMANTIC_TO_AVENDRA", {}) or {}
        except Exception:
            sem_to_av = {}
        # Aggregate boosts per Avendra category with a conservative cap
        cap = 0.4
        for gcat, b in (raw or {}).items():
            try:
                bval = float(b)
            except Exception:
                continue
            av = sem_to_av.get(gcat, gcat)
            prev = float(boosts.get(av, 0.0))
            boosts[av] = max(0.0, min(cap, prev + bval))
        out = {k: v for k, v in boosts.items() if v > 0.0}
        try:
            if hasattr(self, "_cache"):
                self._cache[ck] = dict(out)
        except Exception:
            pass
        return out

    def _discover_assets(self, db: str) -> List[Dict[str, Any]]:
        """Discover tables and extract metadata for classification."""
        try:
            # Cache lookup
            try:
                schema_filter = self._get_schema_filter()
                table_filter = self._get_table_filter()
                ck = f"discover::{db}::{schema_filter or ''}::{table_filter or ''}"
                if hasattr(self, "_cache") and ck in self._cache:
                    v = self._cache.get(ck) or []
                    if isinstance(v, list):
                        return v
            except Exception:
                pass
            # Get filters from global state
            schema_filter = self._get_schema_filter()
            table_filter = self._get_table_filter()

            # Build query with filters
            where_parts = ["TABLE_SCHEMA NOT IN ('INFORMATION_SCHEMA', 'DATA_CLASSIFICATION_GOVERNANCE')",
                          "TABLE_TYPE IN ('BASE TABLE', 'VIEW')"]
            params = {}

            if schema_filter:
                where_parts.append("UPPER(TABLE_SCHEMA) = UPPER(%(sc)s)")
                params["sc"] = schema_filter

            if table_filter:
                where_parts.append("TABLE_NAME ILIKE %(tb)s")
                params["tb"] = f"%{table_filter}%"

            where_sql = " AND ".join(where_parts)

            query = f"""
                SELECT TABLE_CATALOG, TABLE_SCHEMA, TABLE_NAME, TABLE_TYPE, ROW_COUNT, CREATED, LAST_ALTERED, COMMENT
                FROM {db}.INFORMATION_SCHEMA.TABLES
                WHERE {where_sql}
                ORDER BY TABLE_SCHEMA, TABLE_NAME
                LIMIT 100  -- Limit for performance
            """

            rows = snowflake_connector.execute_query(query, params) or []

            assets = []
            for row in rows:
                assets.append({
                    'database': row['TABLE_CATALOG'] if 'TABLE_CATALOG' in row else db,
                    'schema': row['TABLE_SCHEMA'],
                    'table': row['TABLE_NAME'],
                    'full_name': f"{db}.{row['TABLE_SCHEMA']}.{row['TABLE_NAME']}",
                    'comment': row.get('COMMENT'),
                    'created': row.get('CREATED'),
                    'last_altered': row.get('LAST_ALTERED'),
                    'table_type': row.get('TABLE_TYPE'),
                    'row_count': row.get('ROW_COUNT')
                })

            try:
                if hasattr(self, "_cache"):
                    self._cache[ck] = list(assets)
            except Exception:
                pass
            return assets

        except Exception as e:
            logger.error(f"Asset discovery failed: {e}")
            return []

    def _collect_metadata(self, db: str, schema_filter: Optional[str], table_filter: Optional[str], gov_db: str):
        """Collect tables, columns and governance glossary/compliance info."""
        # Tables
        try:
            ck_t = f"meta_tables::{db}::{schema_filter or ''}::{table_filter or ''}"
            if hasattr(self, "_cache") and ck_t in self._cache:
                v = self._cache.get(ck_t)
                if isinstance(v, pd.DataFrame):
                    tables_df = v
                else:
                    raise Exception("no cache df")
            else:
                where_parts = [
                    "TABLE_SCHEMA NOT IN ('INFORMATION_SCHEMA', 'DATA_CLASSIFICATION_GOVERNANCE')",
                    "TABLE_TYPE IN ('BASE TABLE', 'VIEW')",
                ]
                params: Dict[str, Any] = {}
                if schema_filter:
                    where_parts.append("UPPER(TABLE_SCHEMA) = UPPER(%(sc)s)")
                    params["sc"] = schema_filter
                if table_filter:
                    where_parts.append("TABLE_NAME ILIKE %(tb)s")
                    params["tb"] = f"%{table_filter}%"
                where_sql = " AND ".join(where_parts)
                q_tables = f"""
                    SELECT TABLE_CATALOG, TABLE_SCHEMA, TABLE_NAME, TABLE_TYPE, ROW_COUNT, CREATED, LAST_ALTERED, COMMENT
                    FROM {db}.INFORMATION_SCHEMA.TABLES
                    WHERE {where_sql}
                    ORDER BY TABLE_SCHEMA, TABLE_NAME
                """
                t_rows = snowflake_connector.execute_query(q_tables, params) or []
                tables_df = pd.DataFrame(t_rows)
                try:
                    if hasattr(self, "_cache"):
                        self._cache[ck_t] = tables_df
                except Exception:
                    pass
        except Exception:
            tables_df = pd.DataFrame()

        # Columns
        try:
            ck_c = f"meta_columns::{db}::{schema_filter or ''}::{table_filter or ''}"
            if hasattr(self, "_cache") and ck_c in self._cache:
                v2 = self._cache.get(ck_c)
                if isinstance(v2, pd.DataFrame):
                    columns_df = v2
                else:
                    raise Exception("no cache df")
            else:
                q_cols = f"""
                    SELECT 
                        TABLE_SCHEMA,
                        TABLE_NAME,
                        COLUMN_NAME,
                        DATA_TYPE,
                        COMMENT AS COLUMN_COMMENT
                    FROM {db}.INFORMATION_SCHEMA.COLUMNS
                    WHERE TABLE_SCHEMA NOT IN ('INFORMATION_SCHEMA', 'SNOWFLAKE', 'DATA_CLASSIFICATION_GOVERNANCE')
                """
                params2: Dict[str, Any] = {}
                if schema_filter:
                    q_cols += " AND TABLE_SCHEMA = %(sc)s"
                    params2["sc"] = schema_filter
                if table_filter:
                    q_cols += " AND TABLE_NAME ILIKE %(tb)s"
                    params2["tb"] = f"%{table_filter}%"
                q_cols += " ORDER BY TABLE_SCHEMA, TABLE_NAME, ORDINAL_POSITION"
                c_rows = snowflake_connector.execute_query(q_cols, params2) or []
                columns_df = pd.DataFrame(c_rows)
                try:
                    if hasattr(self, "_cache"):
                        self._cache[ck_c] = columns_df
                except Exception:
                    pass
        except Exception:
            columns_df = pd.DataFrame()

        # Glossary / compliance mapping (data-driven from governance config when available)
        try:
            from src.services.ai_classification_service import ai_classification_service as _svc
            try:
                if hasattr(_svc, "_ensure_gov_category_embeddings"):
                    _svc._ensure_gov_category_embeddings()  # type: ignore
            except Exception:
                pass
            comp_map = getattr(_svc, "_gov_cat_compliance", {}) or {}
            thr_map = getattr(_svc, "_gov_cat_thresholds", {}) or {}
            # Optional model metadata threshold for high-risk (fallback to 0.75)
            try:
                cfg = _svc.load_sensitivity_config() or {}
                high_thr = float((cfg.get("model_metadata") or {}).get("high_risk_threshold", 0.75))
            except Exception:
                high_thr = 0.75
            g_rows: List[Dict[str, Any]] = []
            for cat, tags in comp_map.items():
                g_rows.append({
                    "CATEGORY_NAME": str(cat),
                    "IS_HIGH_RISK": bool(float(thr_map.get(cat, 0.0) or 0.0) >= high_thr),
                    "COMPLIANCE_TAGS": ", ".join(tags or [])
                })
            # Fallback to static map only if governance compliance is unavailable
            if not g_rows:
                comp_map2 = getattr(self.ai_service, "_COMPLIANCE_MAP", {}) or {}
                g_rows = [{
                    "CATEGORY_NAME": str(cat),
                    "IS_HIGH_RISK": False,
                    "COMPLIANCE_TAGS": ", ".join(tags or [])
                } for cat, tags in comp_map2.items()]
            glossary_df = pd.DataFrame(g_rows)
        except Exception:
            glossary_df = pd.DataFrame()

        return tables_df, columns_df, glossary_df

    def load_sensitivity_config(self, force_refresh: bool = False, schema_fqn: Optional[str] = None) -> Dict[str, Any]:
        """Lightweight sensitivity config loader used by other services.

        This implementation treats this service as a cache holder only:
        - If _sensitivity_config is already populated and force_refresh is False, it is returned as-is.
        - If force_refresh is True, callers are responsible for repopulating _sensitivity_config from governance tables.
        - If no config is present, an empty dict is returned.
        """
        try:
            cfg = getattr(self, "_sensitivity_config", None)
            if isinstance(cfg, dict) and cfg and not force_refresh:
                return cfg
        except Exception:
            cfg = None

        # If force_refresh is requested, callers (e.g. 3_Classification, seed_governance_service)
        # will typically populate _sensitivity_config after calling this method.
        try:
            if not isinstance(getattr(self, "_sensitivity_config", None), dict):
                self._sensitivity_config = {}
        except Exception:
            self._sensitivity_config = {}

        return self._sensitivity_config


# Export a module-level singleton for backward compatibility
# Many services import `ai_classification_service` expecting a ready-to-use instance
try:
    ai_classification_service = AIClassificationPipelineService()
except Exception:
    # In environments where dependencies are not fully available, fail gracefully
    ai_classification_service = None  # type: ignore

    def _sample_column_values(self, db: str, schema: str, table: str, column: str, sample_rows: int = 100) -> List[Any]:
        """Sample non-null values from a specific column for semantics preview."""
        try:
            ck = f"sample::{db}.{schema}.{table}.{column}::{int(sample_rows or 0)}"
            if hasattr(self, "_cache") and ck in self._cache:
                v = self._cache.get(ck) or []
                if isinstance(v, list):
                    return v
            q = f"""
            SELECT "{column}" AS SAMPLE_VALUE
            FROM "{db}"."{schema}"."{table}"
            SAMPLE ({sample_rows} ROWS)
            WHERE "{column}" IS NOT NULL
            """
            rows = snowflake_connector.execute_query(q) or []
            out = [r.get('SAMPLE_VALUE') for r in rows if r and r.get('SAMPLE_VALUE') is not None]
            try:
                if hasattr(self, "_cache"):
                    self._cache[ck] = list(out)
            except Exception:
                pass
            return out
        except Exception:
            return []

    def _get_schema_filter(self) -> Optional[str]:
        """Get schema filter from session state."""
        try:
            gf = st.session_state.get("global_filters", {}) if hasattr(st, "session_state") else {}
            val = (
                st.session_state.get("schema_filter")
                or (gf.get("schema") if isinstance(gf, dict) else None)
            )
            if val and str(val).strip().upper() in {"ALL", "(ALL)", "*"}:
                return None
            return val
        except Exception:
            return None

    def _get_table_filter(self) -> Optional[str]:
        """Get table filter from session state."""
        try:
            gf = st.session_state.get("global_filters", {}) if hasattr(st, "session_state") else {}
            val = (gf.get("table") if isinstance(gf, dict) else None)
            if val and str(val).strip().upper() in {"ALL", "(ALL)", "*"}:
                return None
            return val
        except Exception:
            return None

    def get_column_detection_results(self, database: str, schema: str, table: str) -> List[Dict[str, Any]]:
        """Get column-level detection results for a specific table using MiniLM embeddings + governance tables."""
        try:
            logger.info(f"Starting column detection for {database}.{schema}.{table}")
            
            # Validate inputs
            if not database or not schema or not table:
                logger.error(f"Invalid inputs: db={database}, schema={schema}, table={table}")
                return []
            
            # Ensure embeddings are initialized
            if self._embedder is None:
                logger.info("Initializing embeddings...")
                self._init_local_embeddings()
            
            if not self._category_centroids:
                logger.info("Initializing centroids...")
                self._init_local_embeddings()
            
            # Auto-tune parameters
            logger.info("Auto-tuning parameters...")
            self._auto_tune_parameters()
            
            # Run column-level classification with governance table integration
            logger.info(f"Running column-level classification with {len(self._category_centroids)} centroids")
            results = self._classify_columns_local(database, schema, table, max_cols=100) or []
            
            # Filter out error rows and ensure we have valid results
            valid_results = [r for r in results if 'error' not in r and r.get('category')]
            logger.info(f"Column detection completed: {len(valid_results)} sensitive columns detected out of {len(results)} total")
            
            return results
        except Exception as e:
            logger.error(f"Column detection failed for {database}.{schema}.{table}: {e}", exc_info=True)
            return []

    def _display_classification_results(self, results: List[Dict[str, Any]]) -> None:
        """Display the classification results in a structured format."""
        st.markdown("#### Classification Results")

        # Filter out errors for display
        successful_results = [r for r in results if 'error' not in r]

        if not successful_results:
            st.warning("No successful classifications to display.")
            return

        # Create display dataframe
        display_data = []
        for result in successful_results:
            asset = result['asset']
            c_val = int(result.get('c', 0) or 0)
            i_val = int(result.get('i', 0) or 0)
            a_val = int(result.get('a', 0) or 0)
            display_data.append({
                "Asset": asset['full_name'],
                "Business Context": result.get('business_context', '')[:100] + '...' if result.get('business_context') and len(result.get('business_context', '')) > 100 else result.get('business_context', ''),
                "Category": result.get('category', 'N/A'),
                "Confidence": f"{result.get('confidence', 0):.1%}",
                "Classification": result.get('label_emoji', result.get('label', 'N/A')),
                "Color": result.get('color', 'N/A'),
                "C": c_val,
                "I": i_val,
                "A": a_val,
                "Compliance": ", ".join(result.get('compliance', [])) if result.get('compliance') else 'N/A',
                "Route": result.get('route', 'N/A'),
                "Status": result.get('application_status', 'N/A')
            })

        results_df = pd.DataFrame(display_data)
        try:
            try:
                theme_base = st.get_option("theme.base")
            except Exception:
                theme_base = "light"
            is_dark = str(theme_base or "").lower() == "dark"
            if is_dark:
                mp = {
                    'Red': '#7f1d1d',
                    'Orange': '#7c2d12',
                    'Yellow': '#7a6e00',
                    'Green': '#14532d',
                    'Gray': '#374151',
                }
                fg = '#ffffff'
            else:
                mp = {
                    'Red': '#ffe5e5',
                    'Orange': '#fff0e1',
                    'Yellow': '#fffbe5',
                    'Green': '#e9fbe5',
                    'Gray': '#f5f5f5',
                }
                fg = '#000000'

            def _apply_classification_style(row: pd.Series):
                col_name = 'Classification'
                col = str(row.get('Color', '') or '')
                bg = mp.get(col, '')
                styles = ['' for _ in row.index]
                try:
                    idx = list(row.index).index(col_name)
                    if bg:
                        styles[idx] = f'background-color: {bg}; color: {fg}; font-weight: 600'
                except Exception:
                    pass
                return styles

            styled = results_df.style.apply(_apply_classification_style, axis=1)
            st.dataframe(styled, width='stretch', hide_index=True)
        except Exception:
            st.dataframe(results_df, width='stretch', hide_index=True)

        # Show detailed results in expandable sections
        with st.expander("Detailed Results", expanded=False):
            for result in successful_results[:10]:  # Limit for performance
                asset = result['asset']
                with st.container():
                    col1, col2 = st.columns([1, 2])

                    with col1:
                        st.markdown(f"**{asset['full_name']}**")
                        st.write(f"**Category:** {result.get('category', 'N/A')}")
                        
                        # Display confidence with enhanced visualization
                        confidence_pct = result.get('confidence', 0) * 100
                        confidence_tier = result.get('confidence_tier', 'Uncertain')
                        if confidence_pct >= 80:
                            st.write(f"**Confidence:** ðŸŸ¢ {confidence_pct:.1f}% ({confidence_tier})")
                        elif confidence_pct >= 60:
                            st.write(f"**Confidence:** ðŸŸ¡ {confidence_pct:.1f}% ({confidence_tier})")
                        else:
                            st.write(f"**Confidence:** ðŸ”´ {confidence_pct:.1f}% ({confidence_tier})")
                        
                        st.write(f"**Classification:** {result.get('label_emoji', result.get('label', 'N/A'))}")
                        st.write(f"**Route:** {result.get('route', 'N/A')}")
                        
                        # Display most relevant compliance frameworks
                        compliance_list = result.get('compliance', [])
                        if compliance_list:
                            # Show top compliance frameworks with icons
                            comp_display = []
                            for comp in compliance_list[:3]:  # Show top 3 compliance frameworks
                                comp_icon = 'ðŸ“‹'
                                if 'GDPR' in comp.upper():
                                    comp_icon = 'ðŸ‡ªðŸ‡º'
                                elif 'CCPA' in comp.upper():
                                    comp_icon = 'ðŸ‡ºðŸ‡¸'
                                elif 'HIPAA' in comp.upper():
                                    comp_icon = 'ðŸ¥'
                                elif 'PCI' in comp.upper():
                                    comp_icon = 'ðŸ’³'
                                elif 'SOX' in comp.upper():
                                    comp_icon = 'ðŸ“Š'
                                comp_display.append(f"{comp_icon} {comp}")
                            st.write(f"**Compliance Frameworks:** {', '.join(comp_display)}")
                        else:
                            st.write(f"**Compliance Frameworks:** None")

                    with col2:
                        business_context = result.get('business_context', '')
                        if business_context:
                            st.write("**Business Context:**")
                            st.text_area("Business Context", value=business_context, height=80, disabled=True, key=f"context_{asset['full_name']}", label_visibility="collapsed")

                        validation_issues = result.get('issues', [])
                        if validation_issues:
                            st.write("**Validation Issues:**")
                            for issue in validation_issues:
                                st.error(f"â€¢ {issue}")

                        sql_preview = result.get('sql_preview')
                        if sql_preview:
                            st.write("**SQL Preview:**")
                            st.code(sql_preview, language='sql')

                    st.divider()
                    
                    # Column-level detection with automatic loading (full-width display)
                    try:
                        table_key = asset['full_name']
                        col_key = f"col_results_{table_key.replace('.', '_')}"
                        col_loading_key = f"col_loading_{table_key.replace('.', '_')}"
                        
                        # Initialize state flags
                        if col_loading_key not in st.session_state:
                            st.session_state[col_loading_key] = False
                        if col_key not in st.session_state:
                            st.session_state[col_key] = None
                        
                        # Auto-load column detection if not already loaded
                        col_rows = st.session_state.get(col_key)
                        if col_rows is None and not st.session_state[col_loading_key]:
                            st.session_state[col_loading_key] = True
                        
                        # Load column detection if triggered
                        if st.session_state[col_loading_key] and col_rows is None:
                            with st.spinner("ðŸ”„ Analyzing columns for sensitive data..."):
                                try:
                                    dbn, scn, tbn = table_key.split('.')
                                    logger.info(f"Fetching column detection for {dbn}.{scn}.{tbn}")
                                    col_rows = self.get_column_detection_results(dbn, scn, tbn)
                                    logger.info(f"Raw column detection results: {len(col_rows) if col_rows else 0} rows returned")
                                    
                                    # Ensure we have a list
                                    if col_rows is None:
                                        col_rows = []
                                    
                                    st.session_state[col_key] = col_rows
                                    st.session_state[col_loading_key] = False
                                    logger.info(f"âœ“ Column detection completed: {len(col_rows)} columns analyzed")
                                except Exception as ce:
                                    logger.error(f"âŒ Column detection error: {ce}", exc_info=True)
                                    st.error(f"âŒ Column detection failed: {ce}")
                                    st.session_state[col_key] = []
                                    st.session_state[col_loading_key] = False
                        
                        # Display column results
                        col_rows = st.session_state.get(col_key, [])
                        logger.info(f"Displaying column results: {len(col_rows) if col_rows else 0} rows in session state")
                        
                        if col_rows and len(col_rows) > 0:
                            logger.info(f"Raw results from _classify_columns_local:")
                            for r in col_rows:
                                logger.info(f"  - {r.get('column')}: {r.get('category')} @ {r.get('confidence_pct')}% (label={r.get('label')})")
                            
                            # Filter for display - show rows with valid category and meaningful confidence
                            # Include Internal, Confidential, Restricted labels (exclude only very low confidence)
                            col_rows_clean = [r for r in col_rows if 'error' not in r and r.get('category') and r.get('confidence_pct', 0) >= 15]
                            
                            logger.info(f"After display filter: {len(col_rows_clean)} rows")
                            for r in col_rows_clean:
                                logger.info(f"  - {r.get('column')}: {r.get('category')} @ {r.get('confidence_pct')}% (label={r.get('label')})")
                            
                            if col_rows_clean and len(col_rows_clean) > 0:
                                st.markdown("#### ðŸ“Š Column-Level Classification Results")
                                
                                # Create display dataframe
                                col_display = []
                                for col in col_rows_clean:
                                    col_display.append({
                                        "Column": col.get('column', 'N/A'),
                                        "Data Type": col.get('data_type', 'N/A'),
                                        "Category": col.get('category', 'N/A'),
                                        "Confidence": f"{col.get('confidence_pct', 0):.1f}%",
                                        "Label": col.get('label', 'N/A'),
                                        "C": col.get('c', 0),
                                        "I": col.get('i', 0),
                                        "A": col.get('a', 0),
                                    })
                                
                                if col_display:
                                    col_df = pd.DataFrame(col_display)
                                    
                                    # Color code by confidence
                                    def _style_confidence(val):
                                        try:
                                            pct = float(val.replace('%', ''))
                                            if pct >= 80:
                                                return 'background-color: #90EE90; color: #000'  # Light green
                                            elif pct >= 60:
                                                return 'background-color: #FFFFE0; color: #000'  # Light yellow
                                            else:
                                                return 'background-color: #FFB6C6; color: #000'  # Light red
                                        except:
                                            return ''
                                    
                                    styled_df = col_df.style.applymap(_style_confidence, subset=['Confidence'])
                                    st.dataframe(styled_df, width='stretch', hide_index=True, use_container_width=True)
                                    
                                    # Summary statistics
                                    confident = sum(1 for c in col_rows_clean if c.get('confidence_pct', 0) >= 80)
                                    likely = sum(1 for c in col_rows_clean if 60 <= c.get('confidence_pct', 0) < 80)
                                    uncertain = sum(1 for c in col_rows_clean if c.get('confidence_pct', 0) < 60)
                                    
                                    st.success(f"âœ… Summary: {confident} Confident (â‰¥80%) | {likely} Likely (60-80%) | {uncertain} Uncertain (<60%)")
                                else:
                                    st.info("â„¹ï¸ No sensitive columns detected in this table.")
                            else:
                                st.info("â„¹ï¸ Column detection completed - no sensitive columns found.")
                        elif col_rows == []:
                            st.info("â„¹ï¸ Column detection completed - no sensitive columns found.")
                    except Exception as e:
                        logger.error(f"Column detection UI error: {e}", exc_info=True)
                        st.error(f"Error displaying column detection: {e}")

        # Summary statistics
        self._display_summary_statistics(successful_results)

    def _display_summary_statistics(self, results: List[Dict[str, Any]]) -> None:
        """Display summary statistics of the classification results."""
        st.markdown("#### Summary Statistics")

        if not results:
            return

        # Calculate statistics
        categories = {}
        routes = {}
        statuses = {}
        confidence_ranges = {'High (â‰¥80%)': 0, 'Medium (60-79%)': 0, 'Low (<60%)': 0}

        for result in results:
            # Categories
            cat = result.get('category', 'Unknown')
            categories[cat] = categories.get(cat, 0) + 1

            # Routes
            route = result.get('route', 'Unknown')
            routes[route] = routes.get(route, 0) + 1

            # Statuses
            status = result.get('application_status', 'Unknown')
            statuses[status] = statuses.get(status, 0) + 1

            # Confidence ranges
            conf = result.get('confidence', 0)
            if conf >= 0.8:
                confidence_ranges['High (â‰¥80%)'] += 1
            elif conf >= 0.6:
                confidence_ranges['Medium (60-79%)'] += 1
            else:
                confidence_ranges['Low (<60%)'] += 1

        # Display in columns
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.markdown("**Categories**")
            for cat, count in sorted(categories.items()):
                st.write(f"{cat}: {count}")

        with col2:
            st.markdown("**Routes**")
            for route, count in sorted(routes.items()):
                st.write(f"{route}: {count}")

        with col3:
            st.markdown("**Statuses**")
            for status, count in sorted(statuses.items()):
                st.write(f"{status}: {count}")

        with col4:
            st.markdown("**Confidence**")
            for range_name, count in confidence_ranges.items():
                st.write(f"{range_name}: {count}")


# Singleton instance
ai_classification_pipeline_service = AIClassificationPipelineService()
